---
title: "Single_source_capture-recapture_models"
author: Piotr Chlebicki, Maciej BerÄ™sewicz
output: 
  rmarkdown::html_vignette:
    toc: true
    number_sections: true
vignette: >
  %\VignetteIndexEntry{Single_source_capture-recapture_models}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\bp}{\boldsymbol{p}}
\newcommand{\blam}{\boldsymbol{\lambda}}
\newcommand{\boeta}{\boldsymbol{\eta}}

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(singleRcapture)
```

# Zero-truncated Poisson model

## Motivation and assumptions

This model was first presented in  P. G. v. d. Heijden et al. 2003, it allows to accommodate heterogeneity of observed units through inclusion of covariates. In this model for population size estimate the following assumptions are made:

+ The dependent variable is generated by Poisson distribution (this assumption also demands that no change in behaviour happens due to being observed)
+ There is no unobserved heterogeneity i.e all important covariates are included in data
+ Independence of probabilities of being observed for each unit

## Derivation of parameters

Let $Y$ be a variable with zero-truncated Poisson distribution described by probability mass function:
\begin{equation}
    \begin{aligned}
    \mathbb{P}(y_{i}|y_{i}>0,\lambda)&=
    \frac{\mathbb{P}(y_{i}|\lambda)}{\mathbb{P}(y_{i}>0|\lambda)}= \frac{\lambda^{y_{i}}}{(\exp(\lambda)-1)y_{i}!} \\
    \mathbb{E}(y_{i}|y_{i}>0,\lambda)&=
    \frac{\lambda}{1-\exp{(-\lambda)}}
    \end{aligned}
\end{equation}
We consider a log-linear regression model given by canonical link function: 
\begin{equation}
    \eta_{i}=
    \ln{\lambda_{i}}=
    \mathbf{x}_{i}\bbeta=
    \beta_{0}+\sum_{k=1}^{p}\beta_{k}x_{ik}=
    \sum_{k=0}^{p}\beta_{k}x_{ik}
\end{equation}
The PMF may be rewritten as a overdispersed exponential family with the overdispersion parameter $\phi=1$ as:
$$\exp\big\{y_{i}\ln{\lambda_{i}} -\ln{(\exp{(\lambda_{i})}-1)}-\ln{y_{i}!}\big\}$$
or in the canonical form:
$$\exp\big\{y_{i}\eta_{i} -\ln{(\exp{(\exp{(\eta_{i})})}-1)}-\ln{y_{i}!}\big\}$$
The log-likelihood function is given by:
\begin{equation}
    \begin{aligned}
    \ell &= \sum_{i=1}^{N_{obs}}
    \left(y_{i}(\boldsymbol{x}_{i}\bbeta)-\exp(\boldsymbol{x}_{i}\bbeta)-
    \ln{(1-\exp(-\exp(\boldsymbol{x}_{i}\bbeta)))}-\ln{(y_{i}!)}\right) \\
    &= \sum_{i=1}^{N_{obs}} \left(y_{i}(\boldsymbol{x}_{i}\bbeta)-\ln{(\exp(\exp(\boldsymbol{x}_{i}\bbeta))-1)}-\ln{(y_{i}!)}\right)
    \end{aligned}
\end{equation}

And the analytical gradient and hessian of the log log-likelihood function with respect to parameter vectors take the forms:
\begin{equation*}
    \frac{\partial \ell}{\partial\bbeta}=
    \begin{pmatrix}
    \sum_{i=1}^{N_{obs}}
    \left(
    y_{i}x_{i0}-x_{i0}
    \exp(\mathbf{x}_{i}\bbeta)-x_{i0}
    \frac{\exp(\mathbf{x}_{i}\bbeta)}{\exp\left(\exp(\mathbf{x}_{i}\bbeta)\right)-1}
    \right) \\
    \vdots \\
    \sum_{i=1}^{N_{obs}}
    \left(
    y_{i}x_{ip}-x_{ip}
    \exp(\mathbf{x}_{i}\bbeta)-x_{ip}
    \frac{\exp(\mathbf{x}_{i}\bbeta)}{\exp\left(\exp(\mathbf{x}_{i}\bbeta)\right)-1}\right)
    \end{pmatrix}
\end{equation*}
\begin{equation*}
    =\begin{pmatrix}
    \sum_{i=1}^{N_{obs}}
    \left(y_{i}x_{i0}-x_{i0}\frac{\exp(\mathbf{x}_{i}\bbeta)}{1-\exp\left(-\exp(\mathbf{x}_{i}\bbeta)\right)}\right) \\
    \vdots \\
    \sum_{i=1}^{N_{obs}}
    \left(y_{i}x_{ip}-x_{ip}\frac{\exp(\mathbf{x}_{i}\bbeta)}{1-\exp\left(-\exp(\mathbf{x}_{i}\bbeta)\right)}\right)
    \end{pmatrix}
\end{equation*}
\begin{equation}
    =\begin{pmatrix}
    \boldsymbol{x}_{(0)}^{T}\boldsymbol{y}-\boldsymbol{x}^{T}_{(0)}\boldsymbol{\mu} \\
    \vdots \\
    \boldsymbol{x}^{T}_{(p)}\boldsymbol{y}-\boldsymbol{x}^{T}_{(p)}\boldsymbol{\mu}
    \end{pmatrix}=
    \boldsymbol{X}^{T}(\boldsymbol{y}-\boldsymbol{\mu})
\end{equation}
$$\frac{\partial^{2} \ell}{\partial\bbeta^{T}\partial\bbeta}=$$
\begin{equation*}
    \begin{pmatrix}
    \sum_{i=1}^{N_{obs}}
    \left(
    x_{iH_{1}}x_{iH_{2}}\exp(\mathbf{x}_{i}\bbeta)
    \frac{\exp{(-\exp(\mathbf{x}_{i}\bbeta))}+\exp(\mathbf{x}_{i}\bbeta) \exp{(-\exp(\mathbf{x}_{i}\bbeta))}-1}
    {\left(1-\exp{(-\exp(\mathbf{x}_{i}\bbeta))}\right)^{2}} 
    \right)
    \end{pmatrix}_{(H_{1},H_{2})}
\end{equation*}
\begin{equation}
    =-\boldsymbol{X}^{T}\frac{\partial\boldsymbol{\mu}}{\partial\blam}
    \frac{\partial\blam}{\partial\bbeta}
\end{equation}
Where 
\begin{equation}
    \frac{\partial\boldsymbol{\mu}}{\partial\blam}=\text{Diag}
    \left(\frac{1}{1-\exp{(-\lambda_{i}})}-
    \frac{\lambda_{i}\exp{(-\lambda_{i})}}
    {(1-\exp{(-\lambda_{i})})^{2}}\right)_{i\in\{1,\dots,N_{obs}\}}
\end{equation}
\begin{equation*}
    =\text{Diag} 
    \left(
    \frac{1}{1-\exp{(-\exp(\mathbf{x}_{i}\bbeta)})}-
    \frac{\exp(\mathbf{x}_{i}\bbeta)\exp{(-\exp(\mathbf{x}_{i}\bbeta))}}
    {(1-\exp{(-\exp(\mathbf{x}_{i}\bbeta))})^{2}}\right)_{i\in\{1,\dots,N_{obs}\}}
\end{equation*}
\begin{equation}
    \frac{\partial\blam}{\partial\bbeta}=
    \boldsymbol{X}^{T}\exp\left(\boldsymbol{X}\bbeta\right)
\end{equation}
\newpage
\subsection{Population size estimation}

For populations where zero value of $y_{i}$ is not observable but has a positive probability of appearing the true population size is unknown. The number of unobserved units is estimated by: 
\begin{equation}
    \boldsymbol{\hat{f}}_{0}=
    \sum_{k=1}^{N}I_{k}\frac{\exp(\exp(\boldsymbol{x}_{i}\hat{\bbeta}))}{1-\exp(-\exp(\boldsymbol{x}_{i}\hat{\bbeta}))}=
    \underbrace{N_{obs}\frac{\exp(-\hat{\lambda})}{1-\exp(-\hat{\lambda})}}_
    {\text{for homogeneous populations}}
\end{equation}
Then the point estimate for the true population size is a sum of number of observed and unobserved units:
$$\hat{N}=N_{obs}+\boldsymbol{\hat{f}}_{0}$$
The correct interpretation for $\hat{N}$ is that it is a estimator of units that have a positive probability of being observed. 

A studentized confidence interval for N:$\hat{N}\pm 1.96SD(\hat{N})$ is constructed after estimating variance of $\hat{N}$ with the help of the law of total variance:
\begin{equation}
    \left(SD(\hat{N})\right)^{2}=
    \text{var}(\hat{N})=
    \mathbb{E}\left(\text{var}\left(
    \hat{N}|I_{1},\dots,I_{n}\right)\right)+
    \text{var}\left(\mathbb{E}(\hat{N}|I_{1},\dots,I_{n})\right)
\end{equation}
Let $\bp(\boldsymbol{x}_{i},\bbeta) = 1-\exp\left(-\exp\left(\bbeta\boldsymbol{x}_{i}\right)\right)$ be a probability of $i$th unit being observed 0 times given a vector of covariates $\boldsymbol{x}_{i}$ and a vector of coefficients $\bbeta$
$$\mathbb{E}\left(\text{var}(\hat{N}|I_{1},\dots,I_{n})\right)\approx$$
\begin{equation}
    \left.
    \left(\frac{\partial}{\partial\bbeta}\sum_{i=1}^{N_{obs}}
    \frac{1}{\bp(\boldsymbol{x}_{i},\bbeta)}\right)^{T}
    \left(-\frac{\partial^{2}\ell}{\partial\bbeta^{T}\partial\bbeta}\right)^{-1}
    \left(\frac{\partial}{\partial\bbeta}\sum_{i=1}^{N_{obs}}
    \frac{1}{\bp(\boldsymbol{x}_{i},\bbeta)}\right)\right|_{\bbeta=\hat{\bbeta}}
\end{equation}
\begin{equation}
    \text{var}(\mathbb{E}(\hat{N}|I_{1},\dots,I_{n}))\approx
    \sum_{i=1}^{N}I_{i}\frac{1-\bp(\boldsymbol{x}_{i},\hat{\bbeta})}
    {\bp^{2}(\boldsymbol{x}_{i},\hat{\bbeta})}
\end{equation}
The derivative present in equation (2.10) is 
\begin{equation}
    \frac{\partial}{\partial\bbeta}
    \left(\sum_{i=1}^{N_{obs}}\frac{1}{\bp(\boldsymbol{x}_{i},\bbeta)}\right) =
   -\boldsymbol{X}^{T}\left(
   \frac{\exp\left(\boldsymbol{X}\bbeta-\exp(\boldsymbol{X}\bbeta)\right)}
   {\left(\boldsymbol{1}-\exp(-\exp{(\boldsymbol{X}\bbeta)})\right)^{2}}\right) 
\end{equation}

## Example

```{r ztpoisson}
summary(
  estimate_popsize(
    formula = capture ~.,
    data = netherlandsimmigrant,
    model = "ztpoisson",
    method = "robust",
    pop.var = "analytic"
  )
)
```

# Zero-truncated negative binomial model

## Motivation
The second listed assumption in the zero-truncated Poisson model was that all important covariates are included in data.


This model is obtained as a modification of negative binomial regression model in parametrization that is derived as overdispersed poisson model.


In Cruyff and P. G. M. v. d. Heijden 2008 it is discussed that in cases when this assumption is violated obtained conditional variance will be higher than conditional mean meaning overdispersion will occur, therefore this model will be more robust with respect to unobserved heterogeneity.

## Derivation of parameters

For the negative binomial distribution characterized by PMF:

\begin{equation}
    \begin{aligned}
    \mathbb{P}(y_{i}|y_{i}>0,\lambda_{i},\alpha)&=
    \frac{\Gamma(y_{i}+\alpha^{-1})}{\Gamma(\alpha^{-1})y_{i}!} 
    \left(\frac{\alpha^{-1}}{\alpha^{-1}+\lambda_{i}}\right)^{\alpha^{-1}}\\
    &\cdot\left(\frac{\lambda_{i}}{\alpha^{-1}+\lambda_{i}}\right)^{y_{i}}
    \frac{1}{\bp(\lambda_{i},\alpha)}\\
    \mathbb{E}\left(y_{i}|y_{i}>0,\lambda_{i},\alpha\right)&= \frac{\lambda_{i}}{\bp(\lambda_{i},\alpha)}
    \end{aligned}
\end{equation}
where $\bp_{i}(\bbeta,\alpha)= \left(1-(1+\alpha\exp(\boldsymbol{x}_{i}\bbeta))^{-\alpha^{-1}}\right)$ is a probability of observing $Y=0$ given parameters.


The log-likelihood function is then given by:
\begin{equation}
    \begin{aligned}
        \ell(\bbeta,\alpha)=\sum_{k=1}^{N_{obs}}&\bigg(\ln{\Gamma(y_{i}+\alpha^{-1})}- \ln{\Gamma(\alpha^{-1})}-\ln{(y_{i}!)}\\
        &-\left(y_{i}+\alpha^{-1}\right)\ln{(1+\alpha\exp(\boldsymbol{x}_{i}\bbeta))}\\
        &+y_{i}\ln{(\alpha\exp(\boldsymbol{x}_{i}\bbeta))}-\ln{\bp_{i}(\bbeta,\alpha)}\bigg)
    \end{aligned}
\end{equation}
With the use of following symbols to simplify the notation 
$$\boldsymbol{S}=\begin{pmatrix}
\frac{1}{1+\alpha \lambda_{1}} \\
\vdots \\
\frac{1}{1+\alpha \lambda_{i}} \\
\vdots \\
\frac{1}{1+\alpha \lambda_{N_{obs}}} \\
\end{pmatrix}
\boldsymbol{\alpha}^{-1}=\begin{pmatrix}
\alpha^{-1} \\
\vdots \\
\alpha^{-1} \\
\vdots \\
\alpha^{-1} \\
\end{pmatrix}\text{same size as }\boldsymbol{S}$$

$$\boldsymbol{G}=\begin{pmatrix}
\frac{1}{1-(1+\alpha \lambda_{1})^{-\alpha^{-1}}} \\
\vdots \\
\frac{1}{1-(1+\alpha \lambda_{i})^{-\alpha^{-1}}} \\
\vdots \\
\frac{1}{1-(1+\alpha\lambda_{N_{obs}})^{-\alpha^{-1}}} \\
\end{pmatrix}
\frac{1}{\boldsymbol{S}^{m}}=\begin{pmatrix}
(1+\alpha \lambda_{1})^{m} \\
\vdots \\
(1+\alpha \lambda_{i})^{m} \\
\vdots \\
(1+\alpha \lambda_{N_{obs}})^{m} \\
\end{pmatrix}$$
The gradient and hessian take forms
\begin{equation}
    \frac{\partial\ell}{\partial\bbeta}=
    \boldsymbol{X}^{T}\left(\left(\boldsymbol{y}+
    \frac{\exp\left(\boldsymbol{X}\bbeta\right)-\boldsymbol{y}}
    {\boldsymbol{S}^{-\alpha^{-1}+1}}\right)\times\boldsymbol{G}\right)
\end{equation}
\begin{equation*}
    \begin{aligned}
        \frac{\partial\ell}{\partial\alpha}=
        &\sum_{k=1}^{N_{obs}}\Bigg(\frac{\partial}{\partial\alpha}
        \left(\ln\Gamma\left(y_{k}+\alpha^{-1}\right)-
        \ln\Gamma\left(\alpha^{-1}\right)\right) +
        \alpha^{-2}\ln{(1+\alpha\exp(\mathbf{x}_{k}\bbeta))} \\
        &+\frac{(1+\exp(\mathbf{x}_{k}\bbeta)\alpha)^{-\alpha^{-1}}
        \left(\frac{\ln{(1+\alpha\exp(\mathbf{x}_{k}\bbeta))}}{\alpha^{2}}-
        \frac{\exp(\mathbf{x}_{k}\bbeta)}
        {(1+\exp(\mathbf{x}_{k}\bbeta)\alpha)\alpha}\right)}
        {1-(1+\exp(\mathbf{x}_{k}\bbeta)\alpha)^{-\alpha^{-1}}} \\
        &+\frac{y}{\alpha}-\frac{(y+\alpha^{-1})\exp(\mathbf{x}_{k}\bbeta)}
        {1+\alpha\exp(\mathbf{x}_{k}\bbeta)}\Bigg)
    \end{aligned}
\end{equation*}
\begin{equation}
    \begin{aligned}
        &\frac{\partial^{2}\ell}{\partial\alpha^{2}}=
        \sum_{k=1}^{N_{obs}}\left(\frac{\partial^{2}}{\partial\alpha^{2}}
        \left(\ln\Gamma\left(y_{k}+\alpha^{-1}\right)-
        \ln\Gamma\left(\alpha^{-1}\right)\right)+
        \frac{2\exp(\mathbf{x}_{k}\bbeta)}
        {\alpha^{2}(1+\exp(\mathbf{x}_{k}\bbeta)\alpha)}\right. \\
        &-\frac{2\ln{(1+\exp(\mathbf{x}_{k}\bbeta)\alpha)}}{\alpha^{3}}+
        \frac{\left(y+\alpha^{-1}\right)\exp(2\mathbf{x}_{k}\bbeta)}
        {(1+\alpha\exp(\mathbf{x}_{k}\bbeta))^{2}} \\
        &+\frac{2\left(\exp(\mathbf{x}_{k}\bbeta)\alpha-
        \ln{(1+\alpha\exp(\mathbf{x}_{k}\bbeta))}
        (1+\alpha\exp(\mathbf{x}_{k}\bbeta))\right)}
        {(1+\alpha\exp(\mathbf{x}_{k}\bbeta))
        \left((1+\alpha\exp(\mathbf{x}_{k}\bbeta))^{\alpha^{-1}}-1\right)\alpha^{3}} \\
        &+\frac{(1+\alpha\exp(\mathbf{x}_{k}\bbeta))^{\alpha^{-1}-1}
        \left(\frac{\exp(\mathbf{x}_{k}\bbeta)}
        {\alpha(1+\alpha\exp(\mathbf{x}_{k}\bbeta))}-
        \frac{\ln{(1+\alpha\exp(\mathbf{x}_{k}\bbeta))}}
        {\alpha^{2}}\right)}
        {\alpha^{2}\left((1+\alpha\exp(\mathbf{x}_{k}\bbeta))^{\alpha^{-1}}-1\right)^{2}}\\
        &\cdot\left(\exp(\mathbf{x}_{k}\bbeta)\alpha-
        \ln{(1+\alpha\exp(\mathbf{x}_{k}\bbeta))}
        (1+\alpha\exp(\mathbf{x}_{k}\bbeta))\right) \\
        &+\frac{\exp(\mathbf{x}_{k}\bbeta)
        \ln{(1+\alpha\exp(\mathbf{x}_{k}\bbeta))}}
        {(1+\alpha\exp(\mathbf{x}_{k}\bbeta))
        \left((1+\alpha\exp(\mathbf{x}_{k}\bbeta))^{\alpha^{-1}}-1\right)\alpha^{2}} \\
        &\left.+\frac{\exp(\mathbf{x}_{k}\bbeta)
        \left(\exp(\mathbf{x}_{k}\bbeta)\alpha-
        \ln{(1+\alpha\exp(\mathbf{x}_{k}\bbeta))}
        (1+\alpha\exp(\mathbf{x}_{k}\bbeta))\right)}
        {\alpha^{2}\left(1+\alpha\exp(\mathbf{x}_{k}\bbeta))^{\alpha^{-1}}-
        1\right)(1+\alpha\exp(\mathbf{x}_{k}\bbeta))^{2}}\right)
    \end{aligned}
\end{equation}
\begin{equation}
    \begin{aligned}
        &\frac{\partial^{2}\ell}{\partial\alpha\partial\bbeta}=
        \frac{\partial^{2}\ell}{\partial\bbeta\partial\alpha}= \\
        -\boldsymbol{X}^{T}&\Bigg(\exp(\boldsymbol{X}\bbeta)\times
        \left(\boldsymbol{y}-\exp(\boldsymbol{X}\bbeta)\right)\times
        \boldsymbol{S}\times\boldsymbol{S}-\exp(\boldsymbol{X}\bbeta)\\
        &\times\bigg(\frac{1}{\boldsymbol{S}^{-1-\alpha^{-1}}}\times
        \Big(\left(1+\alpha^{-1}\right)\exp(\boldsymbol{X}\bbeta)\times\boldsymbol{S}\\
        &-\alpha^{-2}\ln{(\boldsymbol{1}+\alpha\exp(\boldsymbol{X}\bbeta))}\Big)\times\boldsymbol{G}\\
        &+\frac{\alpha^{-2}\ln{\boldsymbol{S}}+\alpha^{-1}\exp(\boldsymbol{X}\bbeta)\times
        \boldsymbol{S}}{\boldsymbol{S}^{-\alpha^{-1}}}\times\frac{1}{\boldsymbol{S}^{-1-\alpha^{-1}}}\times
        \boldsymbol{G}\times\boldsymbol{G}\bigg)\Bigg)
    \end{aligned}
\end{equation}
\begin{equation}
    \begin{aligned}
        \frac{\partial^{2}\ell}{\partial\bbeta^{T}\partial\bbeta}=
        -\boldsymbol{X}^{T}&\Bigg(\boldsymbol{x}_{(i)}\times
        \exp\left(\boldsymbol{X}^{T}\bbeta\right)\times
        \frac{\boldsymbol{1}+\boldsymbol{y}\alpha^{-1}}
        {\left(\boldsymbol{1}+\alpha\exp\left(\boldsymbol{X}^{T}\bbeta\right)\right)^{2}}\\
        &-\boldsymbol{x}_{(i)}\times
        \exp\left(\boldsymbol{X}^{T}\bbeta\right)\times
        \left(\frac{\left(\exp\left(\boldsymbol{X}^{T}\bbeta\right)-\boldsymbol{1}\right)}
        {\boldsymbol{S}^{\alpha^{-1}}}+\boldsymbol{1}\right)\\
        &\times\boldsymbol{S}\times\boldsymbol{S}\times
        \boldsymbol{G}\times\boldsymbol{G}\Bigg)_{i=0}^{p}
    \end{aligned}
\end{equation}
As this model is already quite difficult to fit instead of fitting parameter $\alpha$ 
directly $t=\ln{\alpha}$ is fitted. Derivatives with respect to $t$ may be
easily obtained by applying chain rule to equations with partial derivatives with 
respect to $\alpha$ which results in:
\begin{equation}
    \begin{aligned}
        \frac{\partial\ell}{\partial t}&=
        \frac{\partial\ell}{\partial\alpha}\frac{\partial\alpha}{\partial t}=
        \frac{\partial\ell}{\partial\alpha}\underbrace{\exp(t)}_{\alpha} \\
        \frac{\partial^{2}\ell}{\partial t^{2}}&=
        \underbrace{\exp(t)}_{\alpha}\frac{\partial\ell}{\partial\alpha}+
        \underbrace{\exp(2t)}_{\alpha^{2}}
        \frac{\partial^{2}\ell}{\partial\alpha^{2}} \\
        \frac{\partial^{2}\ell}{\partial t\partial\bbeta}&=
        \underbrace{\exp(t)}_{\alpha}\frac{\partial^{2}\ell}
        {\partial\alpha\partial\bbeta}
    \end{aligned}
\end{equation}
And the log-gamma based functions for the sake of computational convenience, may be
replaced by:
\begin{equation}
    \begin{aligned}
        &\ln\Gamma\left(y_{k}+\alpha^{-1}\right)-
        \ln\Gamma\left(\alpha^{-1}\right)=
        \sum_{j=0}^{y_{k}-1}\ln{\left(j+\alpha^{-1}\right)} \\
        &\frac{\partial}{\partial\alpha}
        \left(\ln\Gamma\left(y_{k}+\alpha^{-1}\right)-
        \ln\Gamma\left(\alpha^{-1}\right)\right)=
        -\alpha^{2}\sum_{j=0}^{y_{k}-1}\frac{1}{j+\alpha^{-1}} \\
        &\frac{\partial^{2}}{\partial\alpha^{2}}
        \left(\ln\Gamma\left(y_{k}+\alpha^{-1}\right)-
        \ln\Gamma\left(\alpha^{-1}\right)\right)=
        \sum_{j=0}^{y_{k}-1}\frac{1+2j\alpha}{\alpha^{2}(1+j\alpha)^{2}}
    \end{aligned}
\end{equation}

## Population size estimation

Estimator of number of unobserved units $\hat{\boldsymbol{f}}_{0}$ and total population size $\hat{N}$ for this model are:
\begin{equation}
    \begin{aligned}
        \hat{\boldsymbol{f}}_{0}&=
        \sum_{k=1}^{N}I_{k}
        \frac{1-\bp_{k}(\hat{\bbeta},\hat{t})}
        {\bp_{k}(\hat{\bbeta},\hat{t})} \\
        \hat{N}&=
        N_{obs}+\hat{\boldsymbol{f}}_{0}=
        \sum_{k=1}^{N}\frac{I_{k}}{\bp_{k}(\hat{\bbeta},\hat{t})}
    \end{aligned}
\end{equation}
The variance of $N$ is estimated by:
\begin{equation}
    \begin{aligned}
        \text{var}(\hat{N})&=
        \mathbb{E}(\text{var}(\hat{N}))+\text{var}(\mathbb{E}(\hat{N})) \\
        \text{var}(\mathbb{E}(\hat{N}))&\approx
        \left.\Theta^{T}\boldsymbol{W}^{-1}\Theta\right|
        _{\bbeta=\hat{\bbeta},t=\hat{t}} \\
        \mathbb{E}(\text{var}(\hat{N}))&=
        \sum_{k=1}^{N}\frac{1-\bp_{k}(\hat{\bbeta},\hat{t})}
        {\bp_{k}(\hat{\bbeta},\hat{t})}\approx
        \sum_{k=1}^{N_{obs}}I_{k}
        \frac{1-\bp_{k}(\hat{\bbeta},\hat{t})}
        {\bp_{k}^{2}(\hat{\bbeta},\hat{t})}
    \end{aligned}
\end{equation}
where
\begin{equation}
    \Theta=
    \begin{pmatrix}
        \frac{\partial}{\partial t}\sum_{k=1}^{N_{obs}}
        \frac{1}{\bp(\lambda_{k},t)} \\
        \frac{\partial}{\partial\bbeta}
        \sum_{k=1}^{N_{obs}}\frac{1}{\bp(\lambda_{k},\alpha)}
    \end{pmatrix}\quad 
    \boldsymbol{W}=-\begin{pmatrix}
        \frac{\partial^{2}\ell}{\partial t^{2}} &
        \frac{\partial^{2}\ell}{\partial\bbeta\partial t}
        \\
        \frac{\partial^{2}\ell}{\partial t\partial\bbeta} &
        \frac{\partial^{2}\ell}{\partial\bbeta^{T}\partial\bbeta}
    \end{pmatrix}
\end{equation}
\begin{equation}
    \begin{aligned}
        \frac{\partial}{\partial\alpha}\frac{1}{\bp_{k}(\bbeta,\alpha)}&=
        \frac{(1+\alpha e^{\bbeta\boldsymbol{x}_{k}})^{\alpha^{-1}-1}}
        {\alpha^{2}\left(1-(1+\alpha\exp(\bbeta\boldsymbol{x}_{k}))^
        {\alpha^{-1}}\right)^{2}}\\
        &\cdot \left((1+\alpha e^{\bbeta\boldsymbol{x}_{k}})
        \ln{(1+\alpha e^{\bbeta\boldsymbol{x}_{k}})}-
        \alpha e^{\bbeta\boldsymbol{x}_{k}}\right)\\
        \frac{\partial}{\partial\alpha}
        \sum_{k=1}^{N_{obs}}\frac{1}{\bp_{k}(\bbeta,\alpha)}&=
        \sum_{k=1}^{N_{obs}}\frac{(1+\alpha 
        e^{\bbeta\boldsymbol{x}_{k}})^{\alpha^{-1}-1}}
        {\alpha^{2}\left(1-(1+\alpha\exp(\bbeta\boldsymbol{x}_{k}))^
        {\alpha^{-1}}\right)^{2}} \\
        &\cdot\left((1+\alpha e^{\bbeta\boldsymbol{x}_{k}})
        \ln{(1+\alpha e^{\bbeta\boldsymbol{x}_{k}})}-
        \alpha e^{\bbeta\boldsymbol{x}_{k}}\right)\\
        \frac{\partial}{\partial t}\sum_{k=1}^{N_{obs}}\frac{1}{\bp_{k}(\bbeta,t)}&= 
        \exp(t)\frac{\partial}{\partial\alpha}
        \sum_{k=1}^{N_{obs}}\frac{1}{\bp(\bbeta,\alpha)}\\
        \frac{\partial}{\partial\bbeta}\frac{1}{\bp_{k}(\bbeta,\alpha)}&=
        -\boldsymbol{x}_{k}\exp(\bbeta\boldsymbol{x}_{k})
        \frac{(1+\alpha\exp(\bbeta\boldsymbol{x}_{k}))^{\alpha^{-1}-1}}
        {\left(1-(1+\alpha\exp(\bbeta\boldsymbol{x}_{k}))^{\alpha^{-1}}\right)^{2}} \\
        \sum_{k=1}^{N_{obs}}\frac{\partial}
        {\partial\bbeta}\frac{1}{\bp_{k}(\bbeta,\alpha)}&=
        -\left(\exp(\boldsymbol{X}^{T}\bbeta)\times\boldsymbol{G}^{2}\times
        \frac{1}{\boldsymbol{S}^{\alpha^{-1}-1}}\right)\boldsymbol{X}^{T}
    \end{aligned}
\end{equation}

## Example

```{r ztnegbin}
summary(
  estimate_popsize(
    formula = TOTAL_SUB ~.,
    data = farmsubmission,
    model = "ztnegbin",
    method = "mle",
    pop.var = "analytic"
  )
)
```

# Zero-truncated geometric model

## Motivation

There are (mostly) numeric problems with negative binomial models, depending on data numerical methods may not be able to fit coefficients or the best estimate for dispersion parameter $\alpha$ may overcompensate for any unobserved heterogeneity, if there is any at all. The zero truncated geometric model is a specific case of zero truncated negative binomial model where $\alpha = 1$, meaning it is still a mixture model derived from zero truncated poisson model so it may better accommodate data with unobserved heterogeneity present.

## Derivation of parameters
The geometric regression is special case of negative binomial regression where the
dispersion parameter $\alpha$ is set to 1.


The PMF of Zero-truncated negative binomial model in the form
\begin{equation}
    \begin{aligned}
        \mathbb{P}(y_{i}|y_{i}>0,\lambda_{i},\alpha)&=
        \frac{\Gamma(y_{i}+\alpha^{-1})}{\Gamma(\alpha^{-1})y_{i}!} 
        \left(\frac{\alpha^{-1}}{\alpha^{-1}+\lambda_{i}}\right)^{\alpha^{-1}} \\
        &\left(\frac{\lambda_{i}}{\alpha^{-1}+\lambda_{i}}\right)^{y_{i}}
        \frac{1}{\bp(\lambda_{i},\alpha)}
    \end{aligned}
\end{equation}
simplifies to:
\begin{equation}
    \begin{aligned}
        \mathbb{P}(y_{i}|y_{i}>0,\lambda_{i},\alpha=1)&=
        \left(\frac{1}{1+\lambda_{i}}\right)
        \left(\frac{\lambda_{i}}{1+\lambda_{i}}\right)^{y_{i}-1}=
        \frac{\lambda_{i}^{y_{i}-1}}{(1+\lambda_{i})^{y_{i}}} \\
        \mathbb{E}(y_{i}|y_{i}>0,\lambda_{i},\alpha=1) &= 1+\lambda_{i}
    \end{aligned}
\end{equation}
The log-likelihood function is therefore expressed as:
\begin{equation}
    \ell=\sum_{k=1}^{N_{obs}}
    \Big((y_{k}-1)\boldsymbol{x}_{k}\bbeta-y_{k}
    \ln\left(1+\exp(\boldsymbol{x}_{k}\bbeta)\right)\Big)
\end{equation}

and gradient and hessian are obtained from zero-truncated negative binomial model (equations (4.4) and (4.6)) by setting $\alpha = 1$ and disregarding $\alpha|\ln\alpha=t$ derivatives and mixed derivatives. In the most simplest form they may be expressed as:

\begin{equation}
    \begin{aligned}
        \frac{\partial\ell}{\partial\bbeta}&=
        \boldsymbol{X}^{T}
        \left(\frac{\boldsymbol{y}-\exp\left(\boldsymbol{X}\bbeta\right)-
        \boldsymbol{1}}{\boldsymbol{1}+
        \exp\left(\boldsymbol{X}\bbeta\right)}\right)\\
        &=\boldsymbol{X}^{T}\left(\frac{\boldsymbol{y}}
        {\boldsymbol{1}+\exp\left(\boldsymbol{X}\bbeta\right)}
        -\boldsymbol{1}\right)
    \end{aligned}
\end{equation}
\begin{equation}
    \frac{\partial^{2}\ell}{\partial\bbeta^{T}\bbeta}=
    -\left(\boldsymbol{x}_{(i)}\times
    \frac{\boldsymbol{y}}{\left(\boldsymbol{1}+
    \exp\left(\boldsymbol{X}\bbeta\right)\right)^{2}}
    \times\exp\left(\boldsymbol{X}\bbeta\right)\right)^{T}\boldsymbol{X}
\end{equation}


## Population size estimation

Estimator of number of unobserved units $\hat{\boldsymbol{f}}_{0}$ and total population size $\hat{N}$ as well as estimator of variance of both estimators var$(\hat{\boldsymbol{f}}_{0})$ in this model is once again derived from zero truncated negative binomial model. The $\bp$ function reduces to
$$\bp(\bbeta)=1-(1+\exp(\boldsymbol{x}_{k}\bbeta))^{-1}=
\frac{\exp(\boldsymbol{x}_{k}\bbeta)}{1+\exp(\boldsymbol{x}_{k}\bbeta)}$$
and therefore estimators can be expressed as
\begin{equation}
    \begin{aligned}
        \hat{\boldsymbol{f}}_{0}&=
        \sum_{k=1}^{N}I_{k}
        \frac{1-\bp(\hat{\bbeta})}
        {\bp_{k}(\hat{\bbeta})}=
        \sum_{k=1}^{N}I_{k}
        \frac{\frac{1}{1+\exp(\boldsymbol{x}_{k}\hat{\bbeta})}}
        {\frac{\exp(\boldsymbol{x}_{k}\hat{\bbeta})}
        {1+\exp(\boldsymbol{x}_{k}\hat{\bbeta})}}=
        \sum_{k=1}^{N_{obs}}
        \frac{1}{\exp(\boldsymbol{x}_{k}\hat{\bbeta})}\\
        \hat{N}&=
        N_{obs}+\hat{\boldsymbol{f}}_{0}=
        \sum_{k=1}^{N_{obs}}\frac{1+\exp(\boldsymbol{x}_{k}\hat{\bbeta})}
        {\exp(\boldsymbol{x}_{k}\hat{\bbeta})}
    \end{aligned}
\end{equation}
The variance estimator in this model obtained by simplifying equations for 
negative binomial model is
\begin{equation}
    \text{var}(\mathbb{E}(\hat{N}))\approx
        \left.\left(\frac{\partial \hat{N}}{\partial\bbeta}\right)^{T}
        \left(-\frac{\partial^{2}\ell}{\partial\bbeta^{T}\bbeta}\right)^{-1}
        \left(\frac{\partial \hat{N}}{\partial\bbeta}\right)
        \right|_{\bbeta=\hat{\bbeta}}
\end{equation}
\begin{equation}
    \begin{aligned}
        \mathbb{E}(\text{var}(\hat{N}))&=
        \sum_{k=1}^{N}\frac{1-\bp_{k}(\hat{\bbeta})}
        {\bp_{k}(\hat{\bbeta})}\approx
        \sum_{k=1}^{N_{obs}}
        \frac{1-\bp_{k}(\hat{\bbeta})}
        {\bp_{k}^{2}(\hat{\bbeta})}=
        \sum_{k=1}^{N_{obs}}
        \frac{1+\exp(\boldsymbol{x}_{k}\bbeta)}
        {\exp(2\boldsymbol{x}_{k}\bbeta)}
    \end{aligned}
\end{equation}
The derivative present in equation (6.7) is beta derivative from (4.14) evaluated at $\alpha=1,t=0$
\begin{equation}
    \frac{\partial\hat{N}}{\partial\bbeta}=
    -\boldsymbol{X}^{T}
    \left(\frac{\exp\left(\boldsymbol{X}\bbeta\right)}
    {\left(\boldsymbol{1}-\left(\boldsymbol{1}+
    \exp\left(\boldsymbol{X}\bbeta\right)\right)\right)^{2}}\right)
\end{equation}
Which simplifies to

$$\frac{\partial\hat{N}}{\partial\bbeta}=
-\boldsymbol{X}^{T}\left(\frac{\boldsymbol{1}}
{\exp\left(\boldsymbol{X}\bbeta\right)}\right)$$

## Example

```{r ztgeom}
summary(
  estimate_popsize(
    formula = TOTAL_SUB ~ .,
    data = farmsubmission,
    model = "ztgeom",
    method = "robust",
    pop.var = "analytic"
  )
)
```

# Zero-truncated one-inflated Poisson model

## Motivation

One of the assumptions present in model presented in P. G. v. d. Heijden et al. 2003 is that being observed does not change the behaviour of unit that is being observed. In case of for example irregular immigrant data or drunk drivers data where units are captured by police it is not difficult to be  sceptical of whether this assumption actually holds. If this assumption is indeed violated we expect significant one inflation to be present in our data, which would increase the number of estimated units. In BÃ¶hning and P. G. M. v. d. Heijden 2019 it is shown that likelihood of zero truncated one inflated distribution and likelihood of zero one truncated distribution differ only by a constant. Using this result we present a way of capturing this behavioral change by considering zero one truncated model, and another one that tries to capture one inflation by estimating it directly.

## Derivation of parameters

For the zero-truncated one-inflated Poisson distribution characterized by it's PMF:
\begin{equation}
    \mathbb{P}(y_{i}|y_{i},\lambda,\omega)=
    \left\{\begin{array}{cc}
    (1-\omega)+\omega h_{+}(y_{i},\lambda)  &  \text{when } y_{i}=1\\
    \omega h_{+}(y_{i},\lambda)             &  \text{when } y_{i}>1
    \end{array}\right.
\end{equation}
Where $h_{+}(y_{i},\lambda)$ is PMF of regular Zero-truncated Poisson distribution and $1-\omega$ is extra mass at $y_{i}=1$.


In BÃ¶hning and P. G. M. v. d. Heijden 2019 it is proven that for $\ell$ the following identity occurs
\begin{equation}
    \ell=
    \boldsymbol{f}_{1}\ln{\frac{\boldsymbol{f}_{1}}{N_{obs}}}+
    (N_{obs}-\boldsymbol{f}_{1})\ln{\left(1-\frac{\boldsymbol{f}_{1}}{N_{obs}}\right)}+
    \ell_{++}
\end{equation}
Where $\ell_{++}$ is the log-likelihood for the zero-one truncated (poisson) distribution and $\ell$ is the log-likelihood of zero truncated one inflated (poisson) distribution.
\begin{equation}
    \begin{aligned}
        \ell_{++}&=
        \sum_{i=1}^{N_{obs}-\boldsymbol{f}_{1}} 
        \Big(y_{i}(\boldsymbol{x}_{i}\bbeta)-
        \exp(\boldsymbol{x}_{i}\bbeta)-\ln{(y_{i}!)}\\
        &-\ln{\left(1-\exp(-\exp(\boldsymbol{x}_{i}\bbeta))-
        \exp(\boldsymbol{x}_{i}\bbeta)
        \exp(-\exp(\boldsymbol{x}_{i}\bbeta))\right)}\Big)
    \end{aligned}
\end{equation}
$\ell_{++}$ is only computed from $\mathcal{L}=(y_{i},\boldsymbol{x}_{i})_{i=1}^{N_{obs}-\boldsymbol{f}_{1}}$ where $y_{i}>1$. Given the form of $\ell_{++}$ the gradient and hessian for this model are:
\begin{equation}
    \frac{\partial\ell_{++}}{\partial\bbeta}=
    \boldsymbol{X}^{T}\left(\boldsymbol{y}-\exp(\boldsymbol{X}\bbeta)-
    \frac{\exp(2\boldsymbol{X}\bbeta)}
    {\exp(\exp(\boldsymbol{X}\bbeta))-\exp(\boldsymbol{X}\bbeta)-\boldsymbol{1}}
    \right)
\end{equation}

\begin{equation}
    \frac{\partial^{2}\ell_{++}}{\partial\bbeta^{T}\partial\bbeta}=
    -\boldsymbol{X}^{T}
    \left(\boldsymbol{x}_{(k)}\times e^{\boldsymbol{X}\bbeta}\times
    \frac{(2+e^{2\boldsymbol{X}\bbeta})e^{e^{\boldsymbol{X}\bbeta}}-
    \boldsymbol{1}-e^{2e^{\boldsymbol{X}\bbeta}}}
    {\left(e^{e^{\boldsymbol{X}\bbeta}}-
    e^{\boldsymbol{X}\bbeta}-\boldsymbol{1}\right)^{2}}
    \right)_{k=0}^{p}
\end{equation}

## Population size estimation

The estimate for number of unobserved units with positive probability of appearing is given by:

\begin{equation}
    \begin{aligned}
    \hat{\boldsymbol{f}}_{0}&=
    \sum_{k=1}^{N}I_{k}\frac{h(0,\lambda_{k})}
    {1-h(0,\lambda_{k})-h(1,\lambda_{k})}=\\
    &=\sum_{k=1}^{N}I_{k}
    \frac{\exp(-\lambda_{k})}
    {1-\exp(-\lambda_{k})-\lambda_{k}\exp(-\lambda_{k})}\\
    &=\sum_{k=1}^{N_{obs}-\boldsymbol{f}_{1}}
    \frac{\exp(-\lambda_{k})}
    {1-\exp(-\lambda_{k})-\lambda_{k}\exp(-\lambda_{k})}
    \end{aligned}
\end{equation}
Where $h$ is PMF of non truncated poisson distribution and $I_{1},\dots,I_{N}$, $I_{k}=1$ when unit k was observed more than 1 time and $I_{k}=0$ otherwise.


So the total number of units is then estimated by:
\begin{equation}
    \begin{aligned}
        \hat{N}&=N_{obs}+\hat{\boldsymbol{f}}_{0}= \\
        \boldsymbol{f}_{1}&+\sum_{k=1}^{N}I_{k}
        \frac{1-\exp(\boldsymbol{x}_{k}\bbeta)\exp(-\exp(\boldsymbol{x}_{k}\bbeta))}
        {1-\exp(-\exp(\boldsymbol{x}_{k}\bbeta))-
        \exp(\boldsymbol{x}_{k}\bbeta)\exp(-\exp(\boldsymbol{x}_{k}\bbeta))}
    \end{aligned}
\end{equation}
The variance of this estimator may be found by applying the law of total variance with indicator variables. The $I_{k}$ has a bernoulli distribution with probability parameter $p_{k}=1-\exp{(-\exp(\boldsymbol{x}_{k}\bbeta))}-\exp(\boldsymbol{x}_{k}\bbeta)\exp{(-\exp(\boldsymbol{x}_{k}\bbeta))}$ and directly from definition it follows that $\sum_{k=1}^{N}I_{k}=N_{obs}-\boldsymbol{f}_{1}$.
\begin{equation*}
    \text{var}(\hat{N})=
    \text{var}\left(\mathbb{E}(\hat{N}|I_{1},\dots,I_{N})\right)+
    \mathbb{E}\left(\text{var}(\hat{N}|I_{1},\dots,I_{N})\right)
\end{equation*}
Since $\hat{N}$ given $I_{1},\dots,I_{N}$ is just a constant we have
\begin{equation}
    \begin{aligned}
        &\mathbb{E}(\hat{N}|I_{1},\dots,I_{N})=
        \boldsymbol{f}_{1} +\\
        &\sum_{k=1}^{N}I_{k}
        \frac{1-\exp(\boldsymbol{x}_{k}\bbeta)\exp(-\exp(\boldsymbol{x}_{k}\bbeta))}
        {1-\exp(-\exp(\boldsymbol{x}_{k}\bbeta))-
        \exp(\boldsymbol{x}_{k}\bbeta)\exp(-\exp(\boldsymbol{x}_{k}\bbeta))}
    \end{aligned}
\end{equation}
and from distribution of indicator variables and properties of variation it follows that
\begin{equation}
    \begin{aligned}
        &\text{var}\left(\mathbb{E}
        (\hat{N}|I_{1},\dots,I_{N})\right)= \\
        &\text{var}\left(\boldsymbol{f}_{1}+\sum_{k=1}^{N}I_{k}
        \frac{1-\exp(\boldsymbol{x}_{k}\bbeta)\exp(-\exp(\boldsymbol{x}_{k}\bbeta))}
        {1-\exp(-\exp(\boldsymbol{x}_{k}\bbeta))-
        \exp(\boldsymbol{x}_{k}\bbeta)\exp(-\exp(\boldsymbol{x}_{k}\bbeta))}
        \right)= \\
        &\sum_{k=1}^{N}
        \frac{\left(1-\exp(\boldsymbol{x}_{k}\bbeta)
        \exp(-\exp(\boldsymbol{x}_{k}\bbeta))\right)^{2}}{p_{k}^{2}}p_{k}(1-p_{k})=\\
        &\sum_{k=1}^{N}\frac{(1-p_{k})}{p_{k}}
        \left(1-\exp(\boldsymbol{x}_{k}\bbeta)\exp(-\exp(\boldsymbol{x}_{k}\bbeta))\right)^{2}
    \end{aligned}
\end{equation}
Since $N$ is not known the term above has to be estimated. Because $\mathbb{E}(I_{k})=p_{k}$ an unbiased estimator is given by 
\begin{equation}
    \begin{aligned}
    &\sum_{k=1}^{N}I_{k}\frac{(1-p_{k})}{p_{k}^{2}}
    \left(1-\exp(\boldsymbol{x}_{k}\bbeta)\exp(-\exp(\boldsymbol{x}_{k}\bbeta))\right)^{2}= \\
    &\sum_{k=1}^{N_{obs}-\boldsymbol{f}_{1}}\frac{(1-p_{k})}{p_{k}^{2}}
    \left(1-\exp(\boldsymbol{x}_{k}\bbeta)\exp(-\exp(\boldsymbol{x}_{k}\bbeta))\right)^{2}
    \end{aligned}
\end{equation}
The second term may be approximated using $\delta$ method
\begin{equation*}
    \text{var}(\hat{N}|I_{1},\dots,I_{n})=
    \left(\frac{\partial}{\partial\bbeta}(\hat{N}|I_{1},I_{2},\dotso,I_{N})\right)^{T}
    \boldsymbol{\Sigma}
    \left(\frac{\partial}{\partial\bbeta}(\hat{N}|I_{1},I_{2},\dotso,I_{N})\right)
\end{equation*}
which is unknown so it has to be estimated by
\begin{equation}
    \left(\frac{\partial}{\partial\bbeta}\hat{N}|I_{1},I_{2},\dotso,I_{N}\right)^{T} \left(-\frac{\partial^{2}\ell}{\partial\bbeta^{T}\partial\bbeta}\right)^{-1} \left(\frac{\partial}{\partial\bbeta}\hat{N}|I_{1},I_{2},\dotso,I_{N}\right)\Big|_{\bbeta=\hat{\bbeta}}
\end{equation}
Since $N|I_{1},\dotso,I_{N}$ is a function of $\bbeta$ the method used to estimated variance analytically requires that, since the delta-method was used, the estimator $\hat{\bbeta}$ is asymptotically normal, estimating $\bbeta$ by maximum-likelihood guarantees this holds. \\
The derivative of $\hat{N}|I_{1},I_{2},\dotso,I_{N}$ with respect to vector of regression coefficients is
\begin{equation}
    \frac{\partial}{\partial\bbeta}\hat{N}=
    -\boldsymbol{X}^{T}\left(
    \frac{\exp(-\exp(\boldsymbol{X}\bbeta))\times\exp(\boldsymbol{X}\bbeta)}
    {\left(\boldsymbol{1}-\exp(-\exp(\boldsymbol{X}\bbeta))-
    \exp(\boldsymbol{X}\bbeta)\exp(-\exp(\boldsymbol{X}\bbeta))\right)^{2}}\right)
\end{equation}

## Proof of lack of bias

It is also useful to know that estimator $\hat{N}$ is (approximately) unbiased. Here proof will be given in a somewhat generalised version. Let $h(y,\lambda_{k})$ be a PMF at point y with parameter $\lambda_{k}$ and the estimator be given by:

\begin{equation}
    \hat{N}=\sum_{k=1}^{N}h(1,\lambda_{k})+
    \sum_{k=1}^{N}
    I_{k}\frac{1-h(1, \lambda_{k})}
    {1-h(0,\lambda_{k})-h(1,\lambda_{k})}
\end{equation}
\begin{theorem}
The $\hat{N}$ estimator of true $N$ is unbiased.
\begin{proof}
Given that indicator variables have bernouli distribution with probability parameter  $p_{k}=1-h(0,\lambda_{k})-h(1,\lambda_{k})$ and that function $h$ evaluated at any point is just a constant using linearity of expected value operator we obtain:
\begin{equation}
    \begin{aligned}
    \mathbb{E}\left(\hat{N}\right)&=\sum_{k=1}^{N}h(1,\lambda_{k}) +
    \mathbb{E}\left(\sum_{k=1}^{N}I_{k}\frac{1-h(1,\lambda_{k})}
    {1-h(0,\lambda_{k})-h(1,\lambda_{k})}\right) \\
    &=\sum_{k=1}^{N}h(1,\lambda_{k})+\sum_{k=1}^{N}\mathbb{E}\left(I_{k}\frac{1-h(1,\lambda_{k})}
    {1-h(0,\lambda_{k})-h(1,\lambda_{k})}\right) \\
    &=\sum_{k=1}^{N}h(1,\lambda_{k})+\sum_{k=1}^{N}\frac{1-h(1,\lambda_{k})}
    {1-h(0,\lambda_{k})-h(1,\lambda_{k})}\mathbb{E}\left(I_{k}\right) \\
    &=\sum_{k=1}^{N}h(1,\lambda_{k})+\sum_{k=1}^{N}\frac{1-h(1,\lambda_{k})}
    {1-h(0,\lambda_{k})-h(1,\lambda_{k})}
    \left(1-h(0,\lambda_{k})-h(1,\lambda_{k})\right) \\
    &=\sum_{k=1}^{N}h(1,\lambda_{k})+\sum_{k=1}^{N}(1-h(1, \lambda_{k}))\\
    &=\sum_{k=1}^{N}h(1,\lambda_{k})+N-\sum_{k=1}^{N}h(1,\lambda_{k})=N
    \end{aligned}
\end{equation}
Which is the target of estimation.
\end{proof}
\end{theorem}
Because $\sum_{k=1}^{N}h(1,\lambda_{k})$ is a unknown quantity it is replaced by $\boldsymbol{f}_{1}$ which is in general a very good estimator.

## Example

```{r zotpoisson}
summary(
  estimate_popsize(
    formula = TOTAL_SUB ~ .,
    data = farmsubmission,
    model = "zotpoisson",
    method = "robust",
    pop.var = "analytic"
  )
)
```

# Zero-truncated one inflated negative binomial model

## Motivation

This model combines one inflation and additional dispersion parameter, so both the justification for zero-truncated negative binomial model and one-inflated zero-truncated Poisson model apply here, meaning this model is able to accommodate change in behaviour on first capture and unobserved heterogeneity/overdisperion in distribution.

## Derivation of parameters

For the zero-truncated one-inflated negative binomial distribution characterized 
by it's PMF:
\begin{equation}
    \mathbb{P}(y_{i}|y_{i},\lambda,\omega)=
    \left\{\begin{array}{cc}
    (1-\omega)+\omega m_{+}(y_{i},\lambda)     &  \text{when } y_{i}=1\\
    \omega m_{+}(y_{i},\lambda)     & \text{when } y_{i}>1
    \end{array}\right.
\end{equation}
Where $m_{+}(y_{i},\lambda)$ is PMF of regular Zero-truncated negative binomial distribution and $1-\omega$ is extra mass at $y_{i}=1$. 


Since identity used in zero truncated poisson model also applies here we shall consider zero-one truncated negative binomial model characterised by the following PMF:

\begin{equation}
    \begin{aligned}
        \mathbb{P}(y_{i}|\alpha,\lambda_{i},y_{i}\geq 2)=
        \frac{\mathbb{P}(y_{i}|\alpha,\lambda_{i})}
        {1-\mathbb{P}(y_{i}=0|\alpha,\lambda_{i})-
        \mathbb{P}(y_{i}=1|\alpha,\lambda_{i})} \\
        =\frac{\Gamma(y_{i}+\alpha^{-1})}{\Gamma(\alpha^{-1})y_{i}!}
        \left(\frac{\alpha^{-1}}{\alpha^{-1}+\lambda_{i}}\right)^{\alpha^{-1}}
        \left(\frac{\lambda_{i}}{\alpha^{-1}+\lambda_{i}}\right)^{y_{i}}
        \frac{1}{\bp(\lambda_{i},\alpha)}
    \end{aligned}
\end{equation}
Where 
$$\bp(\lambda_{i},\alpha)=1-(1+\alpha\lambda_{i})^{-\alpha^{-1}}-\lambda_{i}\left(1+\alpha\lambda_{i}\right)^{-\alpha^{-1}-1}$$
$$\bp(\bbeta,\alpha)=1-(1+\alpha\exp(\boldsymbol{x}_{i}\bbeta))^{-\alpha^{-1}}-\exp(\boldsymbol{x}_{i}\bbeta)\left(1+\alpha\exp(\boldsymbol{x}_{i}\bbeta)\right)^{-\alpha^{-1}-1}$$
Given the PMF the log-likelihood takes the form:
\begin{equation}
    \begin{aligned}
        \ell_{++}=\sum_{k=1}^{N_{obs}-\boldsymbol{f}_{1}}
        &\Big(\ln{\Gamma(y_{i}+\alpha^{-1})}-\ln{(y_{i}!)}-
        \ln{\Gamma(\alpha^{-1})}- \\
        &\left(y_{i}+\alpha^{-1}\right)
        \ln{(1+\alpha\exp(\boldsymbol{x}_{i}\bbeta))}\\
        &+y_{i}\ln{(\alpha\exp(\boldsymbol{x}_{i}\bbeta))}-\ln{\bp(\bbeta,\alpha)}\Big)
    \end{aligned}
\end{equation}
The analytical hessian and gradient, once again the derivatives are computed with respect to $t=\exp(\alpha)$, for this model are as follows:
\begin{equation}
    \begin{aligned}
         \frac{\partial\ell_{++}}{\partial\bbeta}&=
         \boldsymbol{X}^{T}\Bigg(e^{\boldsymbol{X}\bbeta}
         \bigg(\frac{\boldsymbol{y}}{e^{\boldsymbol{X}\bbeta}}-
         \frac{e^{t}\left(\boldsymbol{y}-\boldsymbol{e^{-t}}\right)}
         {\boldsymbol{1}+e^{t}e^{\boldsymbol{X}\bbeta}} \\
         &+\frac{e^{t}e^{\boldsymbol{X}\bbeta}\times\left(-1-e^{-t}\right)
         (\boldsymbol{1}+e^{\boldsymbol{X}\bbeta}e^{t})^{-2-e^{-t}}}
         {\boldsymbol{1}-\left(\boldsymbol{1}+e^{t}e^{\boldsymbol{X}\bbeta}\right)^
         {-e^{-t}}-e^{\boldsymbol{X}\bbeta}
         \left(\boldsymbol{1}+e^{t}e^{\boldsymbol{X}\bbeta}\right)^{-1-e^{-t}}}\bigg)\Bigg)
    \end{aligned}
\end{equation}
\begin{equation}
    \begin{aligned}
         \frac{\partial\ell_{++}}{\partial t}&=
         \sum_{k=1}^{N_{obs}-\boldsymbol{f}_{1}}\Bigg(\frac{\partial}{\partial t}\left(\ln{\left(\frac{\Gamma(y+e^{-t})}{\Gamma(e^{-t})}\right)}\right) + y_{k}\\
         &-\frac{e^{t}e^{\boldsymbol{x}_{k}\bbeta}\left(y_{k}+e^{-t}\right)}
         {1+e^{\boldsymbol{x}_{k}\bbeta}e^{t}}+
         e^{-t}\ln{\left(1+e^{\boldsymbol{x}_{k}\bbeta}e^{t}\right)} \\
         &-\bigg(\left(1+e^{\boldsymbol{x}_{k}\bbeta}e^{t}\right)^{-e^{-t}}
         \left(\frac{e^{\boldsymbol{x}_{k}\bbeta}}
         {1+e^{\boldsymbol{x}_{k}\bbeta}e^{t}}-
         e^{-t}\ln{\left(1+e^{\boldsymbol{x}_{k}\bbeta}e^{t}\right)}\right)\\
         &-e^{\boldsymbol{x}_{k}\bbeta}\left(1+e^{\boldsymbol{x}_{k}\bbeta}e^{t}\right)^{-1-e^{-t}}
         \left(e^{-t}\ln{\left(1+e^{\boldsymbol{x}_{k}\bbeta}e^{t}\right)}
         +\frac{e^{\boldsymbol{x}_{k}\bbeta}\left(-e^{t}-1\right)}
         {1+e^{\boldsymbol{x}_{k}\bbeta}e^{t}}\right)\bigg)\\
         &\bigg/\left(1-\left(1+e^{\boldsymbol{x}_{k}\bbeta}e^{t}\right)^{-e^{-t}}-
         e^{\boldsymbol{x}_{k}\bbeta}\left(1+e^{\boldsymbol{x}_{k}\bbeta}e^{t}\right)^{-1-e^{-t}}\right)\Bigg)
    \end{aligned}
\end{equation}
\begin{equation}
    \begin{aligned}
         \frac{\partial^{2}\ell_{++}}{\partial t^{2}}&= \sum_{k=1}^{N_{obs}-\boldsymbol{f}_{1}}\Bigg(\frac{\partial^{2}}{\partial t^{2}}
         \left(\ln{\left(\frac{\Gamma(y_{k}+e^{-t})}{\Gamma(e^{-t})}\right)}\right)-\frac{e^{t}\left(y_{k}+e^{-t}\right)-1}
         {1+e^{\boldsymbol{x}_{k}\bbeta}e^{t}}\\
         &+\frac{e^{2t}\left(y_{k}+e^{-t}\right)e^{\boldsymbol{x}_{k}\bbeta}}
         {\left(1+e^{\boldsymbol{x}_{k}\bbeta}e^{t}\right)^2}
         +\bigg(\left(-1-e^{t}\right)e^{\boldsymbol{x}_{k}\bbeta}
         \left(1+e^{t}e^{\boldsymbol{x}_{k}\bbeta}\right)^{-2-e^{-t}}\\
         &\cdot\bigg(e^{\boldsymbol{x}_{k}\bbeta}
         \left(1+e^{t}e^{\boldsymbol{x}_{k}\bbeta}\right)^{-e^{-t}-1}
         \left(e^{-t}\ln\left(1+e^{t}e^{\boldsymbol{x}_{k}\bbeta}\right)+
         \frac{\left(-1-e^{t}\right)e^{\boldsymbol{x}_{k}\bbeta}}
         {1+e^{t}e^{\boldsymbol{x}_{k}\bbeta}}\right) \\
         &+\frac{\mathrm{e}^{-a}\ln\left(\mathrm{e}^ax+1\right)-
         \frac{x}{\mathrm{e}^ax+1}}
         {\left(\mathrm{e}^ax+1\right)^{\mathrm{e}^{-a}}}\bigg)\bigg)\\
         &\bigg/\left(1-\left(1+e^{t}e^{\boldsymbol{x}_{k}\bbeta}\right)^
         {-e^{-t}}-e^{\boldsymbol{x}_{k}\bbeta}
         \left(1+e^{t}e^{\boldsymbol{x}_{k}\bbeta}\right)^{-e^{-t}-1}\right)^2 \\
         &+\Bigg(\left(1+e^{t}e^{\boldsymbol{x}_{k}\bbeta}\right)^{-e^{-t}-1}
         \left(e^{-t}\ln\left(1+e^{t}e^{\boldsymbol{x}_{k}\bbeta}\right)+
         \frac{\left(-1-e^{t}\right)e^{\boldsymbol{x}_{k}\bbeta}}
         {1+e^{t}e^{\boldsymbol{x}_{k}\bbeta}}\right)\\
         &+\left(-1-e^{t}\right)e^{\boldsymbol{x}_{k}\bbeta}
         \left(1+e^{t}e^{\boldsymbol{x}_{k}\bbeta}\right)^{-e^{-t}-2}
         \bigg(e^{-t}\ln\left(1+e^{t}e^{\boldsymbol{x}_{k}\bbeta}\right)\\
         &+\frac{\left(-1-e^{t}\right)e^{\boldsymbol{x}_{k}\bbeta}}
         {1+e^{t}e^{\boldsymbol{x}_{k}\bbeta}}\bigg)-
         \left(1+e^{t}e^{\boldsymbol{x}_{k}\bbeta}\right)^{-e^{-t}-1}
         \bigg(e^{-t}\ln\left(1+e^{t}e^{\boldsymbol{x}_{k}\bbeta}\right)\\
         &-\frac{e^{\boldsymbol{x}_{k}\bbeta}}
         {1+e^{t}e^{\boldsymbol{x}_{k}\bbeta}}\bigg)
         +e^{t}e^{\boldsymbol{x}_{k}\bbeta}
         \left(1+e^{t}e^{\boldsymbol{x}_{k}\bbeta}\right)^{-e^{-t}-2}
         +e^{\boldsymbol{x}_{k}\bbeta}\\
         &\cdot\left(1+e^{t}e^{\boldsymbol{x}_{k}\bbeta}\right)^{-e^{-t}-1}
         \left(-\frac{e^{t}}{1+e^{t}e^{\boldsymbol{x}_{k}\bbeta}}-
         \frac{\left(-e^{-t}-1\right)e^{2t}e^{\boldsymbol{x}_{k}\bbeta}}
         {\left(1+e^{t}e^{\boldsymbol{x}_{k}\bbeta}\right)^2}\right)\Bigg) \\
         &\bigg/\left(1-\left(1+e^{t}e^{\boldsymbol{x}_{k}\bbeta}\right)^
         {-e^{-t}}-e^{\boldsymbol{x}_{k}\bbeta}\left(1+e^{t}
         e^{\boldsymbol{x}_{k}\bbeta}\right)^{-e^{-t}-1}\right)\Bigg)
    \end{aligned}
\end{equation}
\begin{equation}
    \begin{aligned}
         \frac{\partial^{2}\ell_{++}}{\partial\bbeta\partial t}&=
         \boldsymbol{X}^{T}\left(\exp(\boldsymbol{X}\bbeta)\times\boldsymbol{V}\right) \\
         \boldsymbol{V}&=-\frac{e^{t}\left(\boldsymbol{y}+\boldsymbol{e^{-t}}\right)}
         {\boldsymbol{1}+e^{t}e^{\boldsymbol{X}\bbeta}}
         +\frac{e^{2t}e^{\boldsymbol{X}\bbeta}\times
         \left(\boldsymbol{y}+\boldsymbol{e^{-t}}\right)}
         {\left(\boldsymbol{1}+e^{t}e^{\boldsymbol{X}\bbeta}\right)^{2}}
         +\frac{\boldsymbol{1}}{\boldsymbol{1}+e^{t}e^{\boldsymbol{X}\bbeta}}\\
         &+\Bigg(\left(-1-e^{t}\right)e^{\boldsymbol{X}\bbeta}
         \times\left(\boldsymbol{1}+e^{t}e^{\boldsymbol{X}\bbeta}\right)^
         {-2-e^{-t}}+(-1-e^{t})e^{\boldsymbol{X}\bbeta}\\
         &\times\left(\boldsymbol{1}+e^{t}e^{\boldsymbol{X}\bbeta}\right)^{-2-e^{-t}}
         \times\bigg(\frac{e^{\boldsymbol{X}\bbeta}e^{t}\left(-2-e^{-t}\right)}
         {\boldsymbol{1}+e^{t}e^{\boldsymbol{X}\bbeta}}\\
         &+e^{-t}\ln{\left(\boldsymbol{1}+e^{t}
         e^{\boldsymbol{X}\bbeta}\right)}\bigg)+e^{\boldsymbol{X}\bbeta}
         \times\left(\boldsymbol{1}+e^{t}e^{\boldsymbol{X}\bbeta}\right)^{-2-e^{-t}}\Bigg)\\
         &\Bigg/\left(\boldsymbol{1}-\left(\boldsymbol{1}+e^{t}
         e^{\boldsymbol{X}\bbeta}\right)^{-e^{-t}}-e^{\boldsymbol{X}\bbeta}
         \left(\boldsymbol{1}+e^{t}e^{\boldsymbol{X}\bbeta}\right)^{-1-e^{-t}}\right) \\
         &-\Bigg((-1-e^{t})e^{\boldsymbol{X}\bbeta}\times
         \left(\boldsymbol{1}+e^{t}e^{\boldsymbol{X}\bbeta}\right)^{-2-e^{-t}}
         \times\bigg(\left(\boldsymbol{1}+e^{t}e^{\boldsymbol{X}\bbeta}\right)^{-e^{-t}} \\
         &\times\left(\frac{e^{\boldsymbol{X}\bbeta}}
         {\boldsymbol{1}+e^{t}e^{\boldsymbol{X}\bbeta}}-e^{-t}\ln\left(
         \boldsymbol{1}+e^{t}e^{\boldsymbol{X}\bbeta}\right)\right)-e^{\boldsymbol{X}\bbeta}\\
         &\times\left(\boldsymbol{1}+e^{t}e^{\boldsymbol{X}\bbeta}\right)^{-1-e^{-t}}
         \times\left(\frac{e^{\boldsymbol{X}\bbeta}
         \left(-1-e^{t}\right)}{\boldsymbol{1}+e^{t}e^{\boldsymbol{X}\bbeta}}
         +e^{-t}\ln\left(\boldsymbol{1}+e^{t}e^{\boldsymbol{X}\bbeta}\right)\right)\Bigg)\\
         &\Bigg/\left(\boldsymbol{1}-\left(\boldsymbol{1}+e^{t}
         e^{\boldsymbol{X}\bbeta}\right)^{-e^{-t}}-
         e^{\boldsymbol{X}\bbeta}\left(\boldsymbol{1}+
         e^{t}e^{\boldsymbol{X}\bbeta}\right)^{-1-e^{-t}}\right)^{2}
    \end{aligned}
\end{equation}
\begin{equation}
    \begin{aligned}
         \frac{\partial^{2}\ell_{++}}{\partial\bbeta^{T}\partial\bbeta}&=
         -\left(\left(\boldsymbol{x}_{(i)}\times\boldsymbol{K}
         \right)_{i=0}^{p}\right)^{T}\boldsymbol{X}\\
         \boldsymbol{K}&=\Bigg(-e^{3t}\boldsymbol{y}\times
         e^{2\boldsymbol{X}\bbeta}\times\left(\left(\boldsymbol{1}+
         e^{t}e^{\boldsymbol{X}\bbeta}\right)^{e^{-t}}-\boldsymbol{1}\right)^{2}\\
         &+e^{2t}e^{\boldsymbol{X}\bbeta}\times\Bigg(e^{2\boldsymbol{X}\bbeta}\times
         \left(\boldsymbol{1}+e^{t}e^{\boldsymbol{X}\bbeta}\right)^{e^{-t}}\\
         &-e^{\boldsymbol{X}\bbeta}\times\left(\left(\boldsymbol{1}+
         e^{t}e^{\boldsymbol{X}\bbeta}\right)^{e^{-t}}-\boldsymbol{1}\right)
         \times\left(\left(\boldsymbol{1}+e^{t}e^{\boldsymbol{X}\bbeta}\right)^
         {e^{-t}}-2\boldsymbol{y}+\boldsymbol{1}\right) \\
         &-2\boldsymbol{y}\times\left(\left(\boldsymbol{1}+
         e^{t}e^{\boldsymbol{X}\bbeta}\right)^{e^{-t}}-\boldsymbol{1}\right)^{2}\Bigg)
         +e^{2\boldsymbol{X}\bbeta}\left(\boldsymbol{1}+
         e^{t}e^{\boldsymbol{X}\bbeta}\right)^{e^{-t}} \\
         &+e^{t}\bigg(e^{3\boldsymbol{X}\bbeta}\times\left(\boldsymbol{1}
         +e^{t}e^{\boldsymbol{X}\bbeta}\right)^{e^{-t}}+
         e^{2\boldsymbol{X}\bbeta}\times\bigg(\left(\boldsymbol{1}+
         e^{t}e^{\boldsymbol{X}\bbeta}\right)^{e^{-t}}\\
         &-\boldsymbol{y}+\boldsymbol{1}\bigg)
         -2e^{\boldsymbol{X}\bbeta}\times\left(\left(\boldsymbol{1}+
         e^{t}e^{\boldsymbol{X}\bbeta}\right)^{-e^{t}}-\boldsymbol{1}\right)\times\\
         &\left(\left(\boldsymbol{1}+e^{t}e^{\boldsymbol{X}\bbeta}\right)^{-e^{t}}-
         \boldsymbol{y}\right)-\boldsymbol{y}\times\left(\left(\boldsymbol{1}+
         e^{t}e^{\boldsymbol{X}\bbeta}\right)^{e^{-t}}-\boldsymbol{1}\right)^{2}\bigg)\\
         &-\left(\left(\boldsymbol{1}+e^{t}e^{\boldsymbol{X}\bbeta}\right)^
         {e^{-t}}-\boldsymbol{1}\right)^{2}\Bigg) \\
         &\Bigg/\Bigg(\left(\boldsymbol{1}+e^{t}e^{\boldsymbol{X}\bbeta}\right)^{2}
         \bigg(\left(\boldsymbol{1}+e^{t}e^{\boldsymbol{X}\bbeta}\right)^
         {e^{-t}}+e^{\boldsymbol{X}\bbeta}\times \\
         &\left(e^{t}\left(\left(\boldsymbol{1}+
         e^{t}e^{\boldsymbol{X}\bbeta}\right)^{e^{-t}}-
         \boldsymbol{1}\right)-\boldsymbol{1}\right)-\boldsymbol{1}\bigg)^{2}\Bigg)
    \end{aligned}
\end{equation}

## Population size estimation

Estimator of number of unobserved units with positive probability of being observed
$\hat{\boldsymbol{f}}_{0}$ and total population size $\hat{N}$ takes analogical form to 
that of Zero one truncated Poisson distribution:
\begin{equation}
    \hat{\boldsymbol{f}}_{0}=
    \sum_{k=1}^{N}I_{k}
    \frac{(1+\alpha\exp(\mathbf{x}_{k}\bbeta))^{-\alpha^{-1}}}
    {1-(1+\alpha\exp(\mathbf{x}_{k}\bbeta))^{-\alpha^{-1}}-
    \exp(\mathbf{x}_{k}\bbeta)
    (1+\alpha\exp(\mathbf{x}_{k}\bbeta))^{-\alpha^{-1}-1}}
\end{equation}
\begin{equation}
    \begin{aligned}
        &\hat{N}=N_{obs}+\hat{\boldsymbol{f}}_{0}=\boldsymbol{f}_{1}+\\
         &\sum_{k=1}^{N}I_{k}
        \frac{1-\exp(\mathbf{x}_{k}\bbeta)
        (1+\alpha\exp(\mathbf{x}_{k}\bbeta))^{-\alpha^{-1}-1}}
        {1-(1+\alpha\exp(\mathbf{x}_{k}\bbeta))^{-\alpha^{-1}}-
        \exp(\mathbf{x}_{k}\bbeta)
        (1+\alpha\exp(\mathbf{x}_{k}\bbeta))^{-\alpha^{-1}-1}}
    \end{aligned}
\end{equation}
And $(I_{k})_{k=1}^{N}$ are constructed in the same way that $(I_{k})_{k=1}^{N}$ are made in zero one truncated poisson model, with the difference that they have different distribution. The variance is estimated by:
\begin{equation}
    \begin{aligned}
        \text{var}(\hat{N})&=
        \mathbb{E}(\text{var}(\hat{N}))+\text{var}(\mathbb{E}(\hat{N})) \\
        \text{var}(\mathbb{E}(\hat{N}))&\approx
        \left.\Theta^{T}\boldsymbol{W}^{-1}
        \Theta\right|_{\bbeta=\hat{\bbeta},t=\hat{t}}\\
        \mathbb{E}(\text{var}(\hat{N}))&=
        \sum_{k=1}^{N}\frac{1-\bp(\hat{\bbeta},\hat{t})}
        {\bp(\hat{\bbeta},\hat{t})}
        \left(1-\exp(\mathbf{x}_{k}\bbeta)
        (1+\exp(t+\mathbf{x}_{k}\bbeta))^{-e^{-t}-1}\right)^{2}\\
        &\approx\sum_{k=1}^{N}I_{k}
        \frac{1-\bp(\hat{\bbeta},\hat{t})}{\bp^{2}(\hat{\bbeta},\hat{t})}
        \left(1-\exp(\mathbf{x}_{k}\bbeta)
         (1+\exp(t+\mathbf{x}_{k}\bbeta))^{-e^{-t}-1}\right)^{2}\\
        &=\sum_{k=1}^{N_{obs}-\boldsymbol{f}_{1}}
        \frac{1-\bp(\hat{\bbeta},\hat{t})}{\bp^{2}(\hat{\bbeta},\hat{t})}
        \left(1-\exp(\mathbf{x}_{k}\bbeta)
         (1+\exp(t+\mathbf{x}_{k}\bbeta))^{-e^{-t}-1}\right)^{2}
    \end{aligned}
\end{equation}
Where $\bp(\bbeta,\alpha)=1-(1+\alpha\exp(\mathbf{x}_{k}\bbeta))^{-\alpha^{-1}}-\exp(\mathbf{x}_{k}\bbeta)(1+\alpha\exp(\mathbf{x}_{k}\bbeta))^{-\alpha^{-1}-1}$.


The second line in equation for $\mathbb{E}(\text{var}(\hat{N}))$ is an unbiased estimation of the first. 


The derivatives present in $\Theta$ vector are:
\begin{equation}
    \begin{aligned}
         \frac{\partial}{\partial t}\sum_{k=1}^{N_{obs}}\frac{1}{\bp(\bbeta, t)} &=
         \sum_{k=1}^{N_{obs}}e^{t}\Bigg(\Bigg(\exp{(\boldsymbol{x}_{k}\bbeta)}
         \left(1+e^{t+\boldsymbol{x}_{k}\bbeta}\right)^{-2 - e^{-t}}\\
         &\cdot\left(1-\left(1+e^{t+\boldsymbol{x}_{k}\bbeta}\right)^{-e^{-t}}-
         e^{\boldsymbol{x}_{k}\bbeta}\left(1+e^{t+\boldsymbol{x}_{k}\bbeta}\right)^
         {-1-e^{-t}}\right) \\
         &\cdot \Big(e^{t+\boldsymbol{x}_{k}\bbeta}\left(e^{t}+1\right)
         -\left(1+e^{t+\boldsymbol{x}_{k}\bbeta}\right)
         \ln\left(1+e^{t+\boldsymbol{x}_{k}\bbeta}\right)\Big)\\
         &-(1-e^{\boldsymbol{x}_{k}\bbeta})\left(1+e^{t+\boldsymbol{x}_{k}\bbeta}
         \right)^{-1-e^{-t}}\Bigg(-e^{\boldsymbol{x}_{k}\bbeta}
         \left(1+e^{t+\boldsymbol{x}_{k}\bbeta}\right)^{-1-e^{-t}}\\
         &\cdot \left(e^{-2t}\ln\left(1+e^{t+\boldsymbol{x}_{k}\bbeta}\right)-
         \left(1+e^{-t}\right)e^{-t}\frac{e^{\boldsymbol{x}_{k}\bbeta}}
         {1+e^{t+\boldsymbol{x}_{k}\bbeta}}\right)\\
         &-\frac{\ln\left(1+e^{t+\boldsymbol{x}_{k}\bbeta}\right)-
         \frac{e^{t+\boldsymbol{x}_{k}\bbeta}}{1+e^{t+\boldsymbol{x}_{k}\bbeta}}}
         {\left(1+e^{t+\boldsymbol{x}_{k}\bbeta}\right)^{e^{-t}}}\Bigg)\Bigg)\\
         &\Bigg/\left(1-\left(1+e^{t+\boldsymbol{x}_{k}\bbeta}\right)^{-e^{-t}}-
         e^{\boldsymbol{x}_{k}\bbeta}\left(1+e^{t+\boldsymbol{x}_{k}\bbeta}\right)^
         {-1-e^{-t}}\right)^{2}\Bigg)
    \end{aligned}
\end{equation}
\begin{equation}
    \begin{aligned}
         \frac{\partial}{\partial\bbeta}\sum_{k=1}^{N_{obs}}\frac{1}{\bp(\bbeta, t)} &=
         \boldsymbol{X}^{T}\Bigg(\exp\left(\boldsymbol{X}\bbeta\right)\times
         \bigg(\bigg(\boldsymbol{1}-\left(\boldsymbol{1}+e^{t}
         \exp\left(\boldsymbol{X}\bbeta\right)\right)^{-e^{-t}}\\
         &-\exp\left(\boldsymbol{X}\bbeta\right)\times\left(\boldsymbol{1}+e^{t}
         \exp\left(\boldsymbol{X}\bbeta\right)\right)^{-1-e^{-t}}\bigg) \\
         &\times\left(\exp\left(\boldsymbol{X}\bbeta\right)-\boldsymbol{1}\right)
         \times\left(\boldsymbol{1}+e^{t}\exp\left(\boldsymbol{X}
         \bbeta\right)\right)^{-2-e^{-t}}\\
         &-\left(1+e^{t}\right)\exp\left(\boldsymbol{X}\bbeta\right)\times
         \left(\boldsymbol{1}+e^{t}
         \exp\left(\boldsymbol{X}\bbeta\right)\right)^{-2-e^{-t}}\\
         &\times\left(\boldsymbol{1}-\exp\left(\boldsymbol{X}\bbeta\right)\times
         \left(\boldsymbol{1}+e^{t}
         \exp\left(\boldsymbol{X}\bbeta\right)\right)^{-1-e^{-t}}\right)\\
         &\Bigg/
         \bigg(\boldsymbol{1}-\left(\boldsymbol{1}+e^{t}
         \exp\left(\boldsymbol{X}\bbeta\right)\right)^{-e^{-t}}-
         \exp\left(\boldsymbol{X}\bbeta\right)\\
         &\times\left(\boldsymbol{1}+e^{t}
         \exp\left(\boldsymbol{X}\bbeta\right)\right)^{-1-e^{-t}}\bigg)^{2}\bigg)\Bigg)
    \end{aligned}
\end{equation}

## Example

```{r zotnegbin}
summary(
  estimate_popsize(
    formula = TOTAL_SUB ~ .,
    data = farmsubmission,
    model = "zotnegbin",
    method = "mle",
    pop.var = "analytic"
  )
)
```

# Zero-truncated one inflated geometric model

## Motivation

The same concerns that are present in zero truncated negative binomial and zero truncated poisson about change in behaviour are also present in zero truncated geometric model, and overcompensation that may be present in zero truncated negative binomial can also occur in zero truncated one inflated negative binomial models. Additionally, from a more practical point of view, the numerical problems present in zero truncated negative binomial are even more pronounced with introduction of inflation parameter/additional one inflation giving yet another reason for consideration of geometric models that is also capable of accommodating any possible one-inflation.

## Derivation of parameters


\begin{equation}
    \begin{aligned}
        \mathbb{P}(y_{i}|\lambda_{i},y_{i}\geq 2)&=
        \frac{\mathbb{P}(y_{i}|\lambda_{i})}
        {1-\mathbb{P}(y_{i}=0|\lambda_{i})-
        \mathbb{P}(y_{i}=1|\lambda_{i})} \\
        &=\frac{\lambda_{i}^{y_{i}-2}}{(1+\lambda_{i})^{y_{i}-1}}\\
        \mathbb{E}(y_{i}|\lambda_{i},y_{i}\geq 2)&=2+\lambda_{i}
    \end{aligned}
\end{equation}
Given the PMF the log-likelihood takes the form:
\begin{equation}
    \begin{aligned}
        \ell_{++}=\sum_{k=1}^{N_{obs}-\boldsymbol{f}_{1}}
        \Big((y_{k}-2)\exp(\boldsymbol{x}_{k}\bbeta)-
        (y_{k}-1)\left(1+\exp(\boldsymbol{x}_{k}\bbeta)\right)\Big)
    \end{aligned}
\end{equation}
By omitting derivatives with respect to $t$ and setting $t=0,\alpha=1$ derivatives in this model are derived as simplified derivatives of zero one truncated negative binomial expressed as:
\begin{equation}
    \frac{\partial\ell_{++}}{\partial\bbeta}=
    \boldsymbol{X}^{T}\left(\frac{\boldsymbol{y}-\boldsymbol{1}}
    {\boldsymbol{1}+\exp\left(\boldsymbol{X}\bbeta\right)}-
    \boldsymbol{1}\right)
\end{equation}
\begin{equation}
    \frac{\partial^{2}\ell_{++}}{\partial\bbeta^{T}\partial\bbeta}=
    -\left(\boldsymbol{x}_{(i)}\times
    \frac{\boldsymbol{y}-\boldsymbol{1}}
    {\left(\exp\left(\boldsymbol{X}\bbeta\right)+
    \boldsymbol{1}\right)^{2}}\times\exp
    \left(\boldsymbol{X}\bbeta\right)\right)^{T}
    \boldsymbol{X}
\end{equation}

## Population size estimation

Estimator of number of unobserved units with positive probability of being observed
$\hat{\boldsymbol{f}}_{0}$ and total population size $\hat{N}$ takes analogical form to 
that of Zero one truncated negative binomial distribution:
\begin{equation}
    \begin{aligned}
        \hat{\boldsymbol{f}}_{0}=
        \sum_{k=1}^{N}I_{k}
        \frac{(1+\exp(\mathbf{x}_{k}\bbeta))^{-1}}
        {1-(1+\exp(\mathbf{x}_{k}\bbeta))^{-1}-
        \exp(\mathbf{x}_{k}\bbeta)
        (1+\exp(\mathbf{x}_{k}\bbeta))^{-2}}
    \end{aligned}
\end{equation}
\begin{equation}
    \begin{aligned}
        \hat{N}&=N_{obs}+\hat{\boldsymbol{f}}_{0}\\&
        =\boldsymbol{f}_{1}+\sum_{k=1}^{N}I_{k}
        \frac{1-\exp(\mathbf{x}_{k}\bbeta)
        (1+\exp(\mathbf{x}_{k}\bbeta))^{-2}}
        {1-(1+\exp(\mathbf{x}_{k}\bbeta))^{-1}-
        \exp(\mathbf{x}_{k}\bbeta)
        (1+\exp(\mathbf{x}_{k}\bbeta))^{-2}} \\
        &=\boldsymbol{f}_{1}+\sum_{k=1}^{N_{obs}-\boldsymbol{f}_{1}}
        \frac{\exp(2\boldsymbol{x}_{k}\bbeta)+\exp(\boldsymbol{x}_{k}\bbeta)+1}
        {\exp(2\boldsymbol{x}_{k}\bbeta)}
    \end{aligned}
\end{equation}
And $(I_{k})_{k=1}^{N}$ are the same as in zero one truncated negative binomial estimator. The variance is estimated by:
\begin{equation}
    \begin{aligned}
        \text{var}(\hat{N})&=
        \mathbb{E}(\text{var}(\hat{N}|I_{1},I_{2},\dotso,I_{N}))+
        \text{var}(\mathbb{E}(\hat{N}|I_{1},I_{2},\dotso,I_{N})) \\
        \text{var}(\mathbb{E}(\hat{N}|I_{1},I_{2},\dotso,I_{N}))&\approx
        \left(\frac{\partial}{\partial\bbeta}\hat{N}|I_{1},I_{2},\dotso,I_{N}\right)^{T}
        \left(-\frac{\partial^{2}\ell}
        {\partial\bbeta^{T}\bbeta}\right)^{-1}\\
        &\left(\frac{\partial}{\partial\bbeta}\hat{N}|I_{1},I_{2},\dotso,I_{N}\right)
        \Bigg|_{\bbeta=\hat{\bbeta}} \\
        \mathbb{E}(\text{var}(\hat{N}|I_{1},I_{2},\dotso,I_{N}))&=
        \sum_{k=1}^{N}\frac{1-\bp(\boldsymbol{x}_{k}\hat{\bbeta})}
        {\bp(\boldsymbol{x}_{k}\hat{\bbeta})}\\&\cdot
        \left(1-\exp(\mathbf{x}_{k}\bbeta)
        (1+\exp(\mathbf{x}_{k}\bbeta))^{-2}\right)^{2}\\
        &\approx\sum_{k=1}^{N}I_{k}
        \frac{1-\bp(\boldsymbol{x}_{k}\hat{\bbeta})}
        {\bp^{2}(\boldsymbol{x}_{k}\hat{\bbeta})}\\&\cdot
        \left(1-\exp(\mathbf{x}_{k}\bbeta)
        (1+\exp(\mathbf{x}_{k}\bbeta))^{-2}\right)^{2}\\
        &=\sum_{k=1}^{N_{obs}}\frac{1-\bp(\boldsymbol{x}_{k}\hat{\bbeta})}
        {\bp^{2}(\boldsymbol{x}_{k}\hat{\bbeta})}\\&\cdot
        \left(1-\exp(\mathbf{x}_{k}\bbeta)
        (1+\exp(\mathbf{x}_{k}\bbeta))^{-2}\right)^{2}
    \end{aligned}
\end{equation}
Where $\bp(\boldsymbol{x}_{k}\bbeta)$ is a probability of observing zero given a linear predictor. The second line in equation (5.11) is an unbiased estimation of the first. \\
The derivative of $\hat{N}$ in the simplified form is
\begin{equation}
    \begin{aligned}
        \frac{\partial\hat{N}|I_{1},I_{2},\dotso,I_{N}}{\partial\bbeta}=
        \boldsymbol{X}^{T}&\Bigg(\Bigg(
        \exp\left(2\boldsymbol{X}\bbeta\right)\times
        \left(\exp\left(2\boldsymbol{X}\bbeta\right)-\boldsymbol{1}\right)\\
        &-2\exp\left(5\boldsymbol{X}\bbeta\right)
        \times\frac{\exp\left(2\boldsymbol{X}\bbeta\right)+
        \exp\left(\boldsymbol{X}\bbeta\right)+\boldsymbol{1}}
        {\left(\boldsymbol{1}+\exp\left(\boldsymbol{X}\bbeta\right)\right)^{2}}\Bigg)\\
        &\Big/\left(\boldsymbol{1}+
        \exp\left(\boldsymbol{X}\bbeta\right)\right)^{5}\Bigg)
    \end{aligned}
\end{equation}

## Example

```{r zotgeom}
summary(
  estimate_popsize(
    formula = TOTAL_SUB ~ .,
    data = farmsubmission,
    model = "zotgeom",
    method = "robust",
    pop.var = "analytic"
  )
)
```

# Zelterman's and Chao's model

## Motivation

Zelterman's and Chao's models of estimating population size are adapted respectively from \cite{zelterman} \cite{chao}, where they were adapted for the cases when covariate information is available. \\
In the context of estimating the size missing population one might reasonably ask if all observed unit of population necessarily have the same effect on unobserved units of population (that have a positive probability of appearing). In many contexts (such as in data on irregular immigrants) units that have been observed many times are less similar to those that have not been observed than those that have only been observed once or twice and therefore may have less influence on unobserved units. The following models are suited to accommodate this disturbance by only considering unit with specific counts specific counts in respectively likelihood function and both respectively and population size estimation. These models are as a byproduct of their derivation highly robust with respect to outlier contamination. It is also worth noting that they are a generalisation of older population size estimators for covariate information.

## Classical versions of estimators

These two estimators originally were designed for homogeneous case (when no covariates are available).

### Zelterman

In \cite{zeltermanog} it is argued that data that is suitable for capture-recapture approaches assumption of Poisson distribution on $Y$ variable may be valid for small ranges of possible values of $Y$ but not necessarily for all values of $Y$. If we consider ranges $[j,j+1]\cap\mathbb{N}$ only frequencies $\boldsymbol{f}_{j}$ and $\boldsymbol{f}_{j+1}$ will be used in estimation of $\lambda$ and later $N$, if poisson distribution is assumed then the following two equation hold:

\begin{equation}
    \begin{aligned}
        \frac{\mathbb{P}(Y=j+1|\lambda)}{\mathbb{P}(Y=j|\lambda)}&=
        \left(\frac{\lambda^{j+1}e^{-\lambda}}{(j+1)!}\right)\Big/
        \left(\frac{\lambda^{j}e^{-\lambda}}{j!}\right)\\
        &=\frac{\lambda\lambda^{j}e^{-\lambda}}{(j+1)j!}
        \frac{j!}{\lambda^{j}e^{-\lambda}}=\frac{\lambda}{j+1}
    \end{aligned}
\end{equation}

\begin{equation}
    \begin{aligned}
        \frac{\mathbb{P}(Y=j+1|\lambda,Y>0)}{\mathbb{P}(Y=j|\lambda,Y>0)}&=
        \left(\frac{\lambda^{j+1}}{(j+1)!\left(e^{\lambda}-1\right)}\right)\Big/
        \left(\frac{\lambda^{j}}{j!\left(e^{\lambda}-1\right)}\right)\\
        &=\frac{\lambda\lambda^{j}}
        {j!\left(e^{\lambda}-1\right)(j+1)}
        \frac{j!\left(e^{\lambda}-1\right)}
        {\lambda^{j}}=\frac{\lambda}{j+1}
    \end{aligned}
\end{equation}

If now we replace probabilities $\mathbb{P}(Y=j+1|\lambda,Y>0), \mathbb{P}(Y=j|\lambda,Y>0)$ by their empirical estimates respectively $\boldsymbol{f}_{j+1}\big/N_{obs},\boldsymbol{f}_{j}\big/N_{obs}$ an estimator for lambda, contingent on choice of $j$ is obtained

\begin{equation}
    \hat{\lambda}_{j}=(j+1)\frac{\boldsymbol{f}_{j+1}\big/N_{obs}}
    {\boldsymbol{f}_{j}\big/N_{obs}}=
    \frac{(j+1)\boldsymbol{f}_{j+1}}{\boldsymbol{f}_{j}}
\end{equation}

Usually case when $j=1$ is considered for a reason that $\boldsymbol{f}_{2}$ and $\boldsymbol{f}_{1}$ are closest to $\boldsymbol{f}_{0}$ which is a target of estimation we have considered. In case when $j=1$ and zelterman estimator $\hat{\lambda}_{1}=2\boldsymbol{f}_{2}\big/\boldsymbol{f}_{1}$ leads to zelterman estimator for $N$ by applying $\hat{\lambda}_{1}$ to homogeneous zero truncated poisson estimator expressed as

\begin{equation}
    \begin{aligned}
    \hat{\boldsymbol{f}}_{0}&=
    N_{obs}\frac{\exp\left(-\hat{\lambda}_{1}\right)}
    {1-\exp\left(-\hat{\lambda}_{1}\right)}=N_{obs}
    \frac{\exp\left(-2\frac{\boldsymbol{f}_{2}}{\boldsymbol{f}_{1}}\right)}
    {1-\exp\left(-2\frac{\boldsymbol{f}_{2}}{\boldsymbol{f}_{1}}\right)} \\
    \hat{N}&=N_{obs}+\hat{\boldsymbol{f}}_{0}=\frac{N_{obs}}
    {1-\exp\left(-2\frac{\boldsymbol{f}_{2}}{\boldsymbol{f}_{1}}\right)}
    \end{aligned}
\end{equation}

In \cite{zelterman} an estimator of $\text{var}\left(\hat{\lambda}_{1}\right)$ is derived and under the assumption that $\hat{\lambda}_{1}$ is normally distributed a confidence interval with $(1 - \alpha)\cdot 100\%$ significance level for $\hat{\lambda}_{1}$ can be expressed as

\begin{equation}
    \left(\hat{\lambda}_{1}-z\left(1-\frac{\alpha}{2}\right)\sqrt{4\frac{\boldsymbol{f}_{2}(\boldsymbol{f}_{1}+\boldsymbol{f}_{2})}{\boldsymbol{f}_{1}^{3}}},
    \hat{\lambda}_{1}+z\left(1-\frac{\alpha}{2}\right)\sqrt{4\frac{\boldsymbol{f}_{2}(\boldsymbol{f}_{1}+\boldsymbol{f}_{2})}{\boldsymbol{f}_{1}^{3}}}\right)
\end{equation}

Where $\sqrt{4\frac{\boldsymbol{f}_{2}(\boldsymbol{f}_{1}+\boldsymbol{f}_{2})}{\boldsymbol{f}_{1}^{3}}}$ is a estimate of standard error and $z = \Phi^{-1}$. \\ One of properties of this estimator is that since only counts $\boldsymbol{f}_{2},\boldsymbol{f}_{1}$ are used it is robust to outlier contamination, a property that is also present in Chao estimator and their generalised versions.

### Chao

If we consider $\lambda$ to be a random variable with PMF $q$ a mixed joint density $m$ for $Y$ and $\lambda$, where $Y|\lambda$ follows a normal distribution, is expressed as

\begin{equation}
    m(y,\lambda)=\mathbb{P}(Y=y|\lambda)q(\lambda)
\end{equation}
and use Cauchy-Schwarz inequality in the form

\begin{equation}
    \left(\int_{0}^{\infty}f(x)g(x)dx\right)^{2}\leq
    \left(\int_{0}^{\infty}(g(x))^{2}dx\right)
    \left(\int_{0}^{\infty}(f(x))^{2}dx\right)
\end{equation}
by substituting $x=\lambda,f(\lambda)=\sqrt{m(0,\lambda)},g(\lambda)=\sqrt{2m(2,\lambda)}$ into (8.7), keeping in mind that
\begin{equation*}
    \begin{aligned}
        m(0,\lambda)&=\mathbb{P}(y=0|\lambda)q(\lambda)=e^{-\lambda}q(\lambda)\\
        m(2,\lambda)&=\mathbb{P}(y=2|\lambda)=\frac{e^{-\lambda}\lambda^{2}}{2}q(\lambda)\\
        (m(1,\lambda))^{2}&=\left(\mathbb{P}(y=1|\lambda)q(\lambda)\right)^{2}=
        \lambda^{2} e^{-2\lambda} (q(\lambda))^{2}=m(0,\lambda)\cdot 2m(2,\lambda)(q(\lambda))^{2}\\
        &\implies m(1,\lambda)=f(\lambda)g(\lambda)
    \end{aligned}
\end{equation*}
A inequality:
\begin{equation}
    \begin{aligned}
        \left(\mathbb{P}(Y=1)\right)^{2}&=
        \left(\int_{0}^{\infty}\lambda e^{-\lambda}q(\lambda)d\lambda\right)^{2}\leq\\
        &\left(2\int_{0}^{\infty}\frac{e^{-\lambda}\lambda^{2}}{2}q(\lambda)d\lambda\right)
        \left(\int_{0}^{\infty}e^{-\lambda}q(\lambda)d\lambda\right)\\
        &=2\mathbb{P}(Y=2)\mathbb{P}(Y=0)
    \end{aligned}
\end{equation}
is obtained. By multiplying both sides of equation above by $N_{obs}^{2}$ and replacing $N_{obs}\mathbb{P}(Y=j)$ by frequencies $\boldsymbol{f}_{j}$ we see that
\begin{equation}
    \boldsymbol{f}_{1}^{2}\leq 2\boldsymbol{f}_{2}\boldsymbol{f}_{0}
    \implies \frac{\boldsymbol{f}_{1}^{2}}{\boldsymbol{f}_{2}}\leq\boldsymbol{f}_{0}
\end{equation}
allowing for a lower bound estimator of our targets $\boldsymbol{f}_{0},N$
\begin{equation*}
    \hat{\boldsymbol{f}}_{0}=\frac{\boldsymbol{f}_{1}^{2}}{\boldsymbol{f}_{2}}
    \implies \hat{N}=N_{obs}+\frac{\boldsymbol{f}_{1}^{2}}{\boldsymbol{f}_{2}}
\end{equation*}
This approach has two important properties, for one it allows some heterogeneity because $\lambda$ is allowed to vary. Secondly from obtained inequality it is reasonable to assume that real $N$ is greater than $\hat{N}$ giving us a reliable lower bound estimator.

## Derivation of parameters

The regression described bellow is used in Zelterman and Chao model. The difference in these model's is not found in associated regression but only with population size estimation. Let $Y$ be a variable following a Poisson distribution. We consider a new variable $Z$ with the property that, for $Y$ truncated to just 1 and 2 counts the following occurs

\begin{equation}
    Z = \left\{\begin{array}{cc}
    0     & \text{if }Y = 1  \\
    1     & \text{if }Y = 2
    \end{array}\right.
\end{equation}

It is clear that $Z$ follows a bernouli distribution with probability parameter $p_{k}=\frac{\frac{\lambda}{2}}{1+\frac{\lambda}{2}}$. For this model we consider a regression with the following link function:

\begin{equation}
    \text{logit}(p_{k})=
    \ln\left(\frac{\lambda_{k}}{2}\right)=
    \bbeta\mathbf{x}_{k}=\eta_{k}
\end{equation}
The associated likelihood function is expressed by:
\begin{equation}
    L=\prod_{k=1}^{\boldsymbol{f}_{1}+\boldsymbol{f}_{2}}
    \left(\frac{\exp(\eta_{k})}{1+\exp(\eta_{k})}\right)^{z_{k}}
    \left(\frac{1}{1+\exp(\eta_{k})}\right)^{1-z_{k}}
\end{equation}
\begin{equation}
    \ell=\sum_{k=1}^{\boldsymbol{f}_{1}+\boldsymbol{f}_{2}}
    \left(z_{k}\ln\left(\frac{\exp(\eta_{k})}{1+\exp(\eta_{k})}\right)+
    (1-z_{k})\ln\left(\frac{1}{1+\exp(\eta_{k})}\right)\right)
\end{equation}
\begin{equation*}
    =\sum_{k=1}^{\boldsymbol{f}_{1}+\boldsymbol{f}_{2}}
    \left(z_{k}\ln\left(\frac{\exp(\mathbf{x}_{k}\bbeta)}
    {1+\exp(\mathbf{x}_{k}\bbeta)}\right)+
    (1-z_{k})\ln\left(\frac{1}{1+\exp(\mathbf{x}_{k}\bbeta)}\right)\right)
\end{equation*}
As it is just a modified binomial regression the derivatives are 
well know, but they're provided for the sake of completion:
\begin{equation}
    \frac{\partial\ell}{\partial\bbeta}=
    \boldsymbol{X}^{T}\left(
    \frac{\exp\left(\boldsymbol{X}\bbeta\right)\times
    (\boldsymbol{z}-1)+\boldsymbol{z}}
    {1+\exp\left(\boldsymbol{X}\bbeta\right)}\right)
\end{equation}
\begin{equation}
    \frac{\partial^{2}\ell}{\partial\bbeta^{T}\partial\bbeta}=
    -\left(\boldsymbol{x}_{(i)}\times
    \frac{\exp\left(\boldsymbol{X}\bbeta\right)}
    {\left(1+\exp\left(\boldsymbol{X}\bbeta\right)\right)^{2}}\right)^{T}
    \boldsymbol{X}
\end{equation}

## Zelterman

### Population size estimation

The true population size is estimated by:
\begin{equation}
    \hat{N}=
    \sum_{k=1}^{N}\frac{I_{k}}
    {1-\exp\left(-2\exp\left(\boldsymbol{x}_{k}\hat{\bbeta}\right)\right)}
\end{equation}
\begin{equation*}
    \boldsymbol{\hat{f}}_{0}=
    \sum_{k=1}^{N}I_{k}
    \frac{\exp\left(-2\exp\left(\boldsymbol{x}_{k}\hat{\bbeta}\right)\right)}
    {1-\exp\left(-2\exp\left(\boldsymbol{x}_{k}\hat{\bbeta}\right)\right)}
\end{equation*}
It is important to note that although the model is fitted only using one and two counts all observations are used in this model to produce estimation of population size.


Confidence intervals are constructed in the same way that the previously described models had their confidence intervals constructed. In estimating variance and using studentized form of confidence intervals.
\begin{equation}
    \text{var}(\hat{N})=\mathbb{E}(\text{var}(\hat{N}|I_{1},\dotso,I_{N}))+
    \text{var}(\mathbb{E}(\hat{N}|I_{1},\dotso,I_{N}))
\end{equation}
\begin{equation}
    \text{var}(\mathbb{E}(\hat{N}|I_{1},\dotso,I_{N}))=
    \sum_{k=1}^{N_{obs}}
    \frac{\exp\left(-2\exp\left(\boldsymbol{x}_{k}\hat{\bbeta}\right)\right)}
    {\left(1-\exp\left(-2\exp\left(\boldsymbol{x}_{k}\hat{\bbeta}
    \right)\right)\right)^{2}}
\end{equation}
\begin{equation}
    \mathbb{E}(\text{var}(\hat{N}|I_{1},\dotso,I_{N}))\approx
    \left(\frac{\partial\hat{N}}{\partial\bbeta}\right)^{T}
    \left(-\frac{\partial^{2}\ell}
    {\partial\bbeta^{T}\partial\bbeta}\right)^{-1}
    \left(\frac{\partial\hat{N}}{\partial\bbeta}\right)\Bigg|_{\bbeta=\hat{\bbeta}}
\end{equation}
Where:
\begin{equation}
    \frac{\partial\hat{N}}{\partial\bbeta}=-\boldsymbol{X}^{T}\left(
    \frac{\exp\left(-2\exp\left(\boldsymbol{X}^{T}\bbeta\right)\right)}
    {\left(1-\exp\left(-2\exp\left(\boldsymbol{X}^{T}\bbeta
    \right)\right)\right)^{2}}
    \times 2\exp\left(\boldsymbol{X}^{T}\bbeta\right)\right)
\end{equation}

## Chao

### Population size estimation

The true population size is estimated by:
\begin{equation}
    \hat{N}=N_{obs}+
    \sum_{k=1}^{\boldsymbol{f}_{1}+\boldsymbol{f}_{2}}\frac{I_{k}}
    {2\exp\left(\boldsymbol{x}_{k}\hat{\bbeta}\right)+
    2\exp\left(2\boldsymbol{x}_{k}\hat{\bbeta}\right)}
\end{equation}
With confidence interval constructed the same way as in previous models, by breaking variance of estimator into two parts by law of total variance and using multivariate delta method to estimate resulting term.
\begin{equation}
    \text{var}(\hat{N})=\mathbb{E}(\text{var}(\hat{N}|I_{1},\dotso,I_{N}))+
    \text{var}(\mathbb{E}(\hat{N}|I_{1},\dotso,I_{N}))
\end{equation}
\begin{equation}
    \text{var}(\mathbb{E}(\hat{N}|I_{1},\dotso,I_{N}))\approx
    \sum_{k=1}^{\boldsymbol{f}_{1}+\boldsymbol{f}_{2}}
    (1-p_{k})\left(1+
    \frac{\exp\left(-2\exp\left(\boldsymbol{x}_{k}\hat{\bbeta}\right)\right)}
    {p_{k}}\right)^{2}
\end{equation}
Where:
\begin{equation*}
    \begin{aligned}
    p_{k}&=
    2\exp\left(\boldsymbol{x}_{k}\hat{\bbeta}\right)
    \exp\left(-2\exp\left(\boldsymbol{x}_{k}\hat{\bbeta}\right)\right) \\
    &+2\exp\left(2\boldsymbol{x}_{k}\hat{\bbeta}\right)
    \exp\left(-2\exp\left(\boldsymbol{x}_{k}\hat{\bbeta}\right)\right)
    \end{aligned}
\end{equation*}
\begin{equation}
    \mathbb{E}(\text{var}(\hat{N}|I_{1},\dotso,I_{N}))\approx
    \left(\frac{\partial\hat{N}}{\partial\bbeta}\right)^{T}
    \left(-\frac{\partial^{2}\ell}
    {\partial\bbeta^{T}\partial\bbeta}\right)^{-1}
    \left(\frac{\partial\hat{N}}{\partial\bbeta}\right)\Bigg|_{\bbeta=\hat{\bbeta}}
\end{equation}
\begin{equation}
    \frac{\partial\hat{N}}{\partial\bbeta}=
    -\boldsymbol{X}^{T}
    \left(\frac{2\exp\left(\boldsymbol{X}\bbeta\right)+
    4\exp\left(2\boldsymbol{X}\bbeta\right)}
    {\left(2\exp\left(\boldsymbol{X}\bbeta\right)+
    2\exp\left(2\boldsymbol{X}\bbeta\right)\right)^{2}}\right)
\end{equation}

## Examples

```{r zelterman}
summary(
  estimate_popsize(
    formula = capture ~ .,
    data = netherlandsimmigrant,
    model = "zelterman",
    method = "robust",
    pop.var = "analytic"
  )
)
```

```{r chao}
summary(
  estimate_popsize(
    formula = capture ~ .,
    data = netherlandsimmigrant,
    model = "chao",
    method = "robust",
    pop.var = "analytic"
  )
)
```
